{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Adult* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/2/adult). Extract the data files into the subdirectory: `../05_src/data/adult/` (relative to `./05_src/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Assuming that the files `adult.data` and `adult.test` are in `../05_src/data/adult/`, then you can use the code below to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import prerequisite python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, log_loss, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "    'native-country', 'income'\n",
    "]\n",
    "adult_dt = (pd.read_csv(r'C:\\Users\\ibast\\DSI_production\\05_src\\adult\\adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data:\n",
    "\n",
    "+ Create a dataframe `X` that holds the features (all columns that are not `income`).\n",
    "+ Create a dataframe `Y` that holds the target data (`income`).\n",
    "+ From `X` and `Y`, obtain the training and testing data sets:\n",
    "\n",
    "    - Use a train-test split of 70-30%. \n",
    "    - Set the random state of the splitting function to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  income  \n",
       "0          2174             0              40   United-States       0  \n",
       "1             0             0              13   United-States       0  \n",
       "2             0             0              40   United-States       0  \n",
       "3             0             0              40   United-States       0  \n",
       "4             0             0              40            Cuba       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (22792, 14)\n",
      "X_test shape: (9769, 14)\n",
      "Y_train shape: (22792,)\n",
      "Y_test shape: (9769,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have already created the adult_dt DataFrame as shown previously\n",
    "\n",
    "# Step 1: Create the feature (X) and target (Y) dataframes\n",
    "X = adult_dt.drop(columns=['income'])  # Drop the target column\n",
    "Y = adult_dt['income']  # Select the target column\n",
    "\n",
    "# Step 2: Split the data into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets to confirm\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\n",
    "print(f\"Y_test shape: {Y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random States\n",
    "\n",
    "Please comment: \n",
    "\n",
    "+ What is the [random state](https://scikit-learn.org/stable/glossary.html#term-random_state) of the [splitting function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)? \n",
    "+ Why is it [useful](https://en.wikipedia.org/wiki/Reproducibility)?\n",
    "\n",
    "The random_state parameter in the train_test_split function (and other scikit-learn functions) sets a seed for the random number generator used to split data into training and test sets. This parameter is essential for reproducibility, as it ensures the random processes, like shuffling and splitting, yield the same results each time the code runs with the same data.\n",
    "\n",
    "Why is random_state useful?\n",
    "Reproducibility: Setting random_state allows other researchers or developers to reproduce the exact splits or random processes in your experiments. Reproducibility is essential in scientific research and data science, as it ensures that others can verify, validate, and build upon your work under identical conditions.\n",
    "\n",
    "Model Comparison: When testing or comparing different models, itâ€™s essential to use the same training and test splits to ensure that performance differences are due to the models themselves, not variations in the data split. A fixed random_state guarantees consistent data splits for fair comparison.\n",
    "\n",
    "Debugging: Consistent splits make it easier to debug or fine-tune model training, as variations due to data splits are eliminated, allowing you to focus on other variables in the modeling process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Comment here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create a [Column Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) that treats the features as follows:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * Apply [KNN-based imputation for completing missing values](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html):\n",
    "        \n",
    "        + Consider the 7 nearest neighbours.\n",
    "        + Weight each neighbour by the inverse of its distance, causing closer neigbours to have more influence than more distant ones.\n",
    "    * [Scale features using statistics that are robust to outliers](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler).\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * Apply a [simple imputation strategy](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer):\n",
    "\n",
    "        + Use the most frequent value to complete missing values, also called the *mode*.\n",
    "\n",
    "    * Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html):\n",
    "        \n",
    "        + Handle unknown labels if they exist.\n",
    "        + Drop one column for binary variables.\n",
    "    \n",
    "    \n",
    "The column transformer should look like this:\n",
    "\n",
    "![](./images/assignment_2__column_transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = adult_dt.drop('income', axis=1)\n",
    "y = adult_dt['income']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "numerical_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "    \n",
    "        ('num', \n",
    "         Pipeline(steps=[\n",
    "             ('knn_imputer', KNNImputer(n_neighbors=7, weights='distance')), \n",
    "             ('robust_scaler', RobustScaler())  \n",
    "         ]), numerical_cols),\n",
    "        \n",
    "        # Categorical pipeline\n",
    "        ('cat', \n",
    "         Pipeline(steps=[\n",
    "             ('simple_imputer', SimpleImputer(strategy='most_frequent')),  # Mode Imputation\n",
    "             ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))  # One-hot Encoding\n",
    "         ]), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "encoded_feature_names = ohe.get_feature_names_out(categorical_cols)\n",
    "\n",
    "feature_names = numerical_cols + list(encoded_feature_names)\n",
    "\n",
    "X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=feature_names, index=X_train.index)\n",
    "X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=feature_names, index=X_test.index)\n",
    "\n",
    "# Display the transformed DataFrames\n",
    "print(X_train_transformed_df.head())\n",
    "print(X_test_transformed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `classifier` and assign a [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to it.\n",
    "\n",
    "The pipeline looks like this:\n",
    "\n",
    "![](./images/assignment_2__pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 1 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = adult_dt.drop('income', axis=1)\n",
    "y = adult_dt['income']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# numerical and categorical columns\n",
    "numerical_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Numerical pipeline\n",
    "        ('num', \n",
    "         Pipeline(steps=[\n",
    "             ('knn_imputer', KNNImputer(n_neighbors=7, weights='distance')),  \n",
    "             ('robust_scaler', RobustScaler()) \n",
    "         ]), numerical_cols),\n",
    "        \n",
    "        ('cat', \n",
    "         Pipeline(steps=[\n",
    "             ('simple_imputer', SimpleImputer(strategy='most_frequent')), \n",
    "             ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))  \n",
    "         ]), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# model pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),             \n",
    "    ('classifier', RandomForestClassifier())       \n",
    "])\n",
    "\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "print(y_pred[:10]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Evaluate the model pipeline using [`cross_validate()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html):\n",
    "\n",
    "+ Measure the following [preformance metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values): negative log loss, ROC AUC, accuracy, and balanced accuracy.\n",
    "+ Report the training and validation results. \n",
    "+ Use five folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ibast\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\ibast\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEG_LOG_LOSS - Training and Validation Results:\n",
      "Training neg_log_loss: [-0.08070598 -0.08195462 -0.08144901 -0.08198176 -0.08195931]\n",
      "Validation neg_log_loss: [-0.39445382 -0.35021271 -0.39649306 -0.36466    -0.38524183]\n",
      "Average Training neg_log_loss: -0.0816\n",
      "Average Validation neg_log_loss: -0.3782\n",
      "\n",
      "ROC_AUC - Training and Validation Results:\n",
      "Training roc_auc: [1. 1. 1. 1. 1.]\n",
      "Validation roc_auc: [0.89509928 0.90506718 0.90207796 0.90581613 0.9047243 ]\n",
      "Average Training roc_auc: 1.0000\n",
      "Average Validation roc_auc: 0.9026\n",
      "\n",
      "ACCURACY - Training and Validation Results:\n",
      "Training accuracy: [1.         0.99995201 1.         1.         0.99990403]\n",
      "Validation accuracy: [0.84702495 0.84971209 0.85355086 0.85563448 0.86004991]\n",
      "Average Training accuracy: 1.0000\n",
      "Average Validation accuracy: 0.8532\n",
      "\n",
      "BALANCED_ACCURACY - Training and Validation Results:\n",
      "Training balanced_accuracy: [1.         0.99990032 1.         1.         0.99980064]\n",
      "Validation balanced_accuracy: [0.76364471 0.77385652 0.77012065 0.7755871  0.78285164]\n",
      "Average Training balanced_accuracy: 0.9999\n",
      "Average Validation balanced_accuracy: 0.7732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, log_loss, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "scoring = {\n",
    "    'neg_log_loss': 'neg_log_loss',\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'accuracy': 'accuracy',\n",
    "    'balanced_accuracy': 'balanced_accuracy'\n",
    "}\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    model_pipeline,      \n",
    "    X_train,            \n",
    "    y_train,            \n",
    "    cv=5,                \n",
    "    scoring=scoring,    \n",
    "    return_train_score=True  \n",
    ")\n",
    "for metric in scoring.keys():\n",
    "    print(f\"\\n{metric.upper()} - Training and Validation Results:\")\n",
    "    print(f\"Training {metric}: {cv_results['train_' + metric]}\")\n",
    "    print(f\"Validation {metric}: {cv_results['test_' + metric]}\")\n",
    "    print(f\"Average Training {metric}: {cv_results['train_' + metric].mean():.4f}\")\n",
    "    print(f\"Average Validation {metric}: {cv_results['test_' + metric].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the fold-level results as a pandas data frame and sorted by negative log loss of the test (validation) set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of each metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-level Results Sorted by Negative Log Loss:\n",
      "    fit_time  score_time  test_neg_log_loss  train_neg_log_loss  test_roc_auc  \\\n",
      "1  14.667752    0.277882          -0.350213           -0.081955      0.905067   \n",
      "3  15.665269    0.288437          -0.364660           -0.081982      0.905816   \n",
      "4  16.953823    0.299644          -0.385242           -0.081959      0.904724   \n",
      "0  18.869560    0.292064          -0.394454           -0.080706      0.895099   \n",
      "2  14.447720    0.305879          -0.396493           -0.081449      0.902078   \n",
      "\n",
      "   train_roc_auc  test_accuracy  train_accuracy  test_balanced_accuracy  \\\n",
      "1            1.0       0.849712        0.999952                0.773857   \n",
      "3            1.0       0.855634        1.000000                0.775587   \n",
      "4            1.0       0.860050        0.999904                0.782852   \n",
      "0            1.0       0.847025        1.000000                0.763645   \n",
      "2            1.0       0.853551        1.000000                0.770121   \n",
      "\n",
      "   train_balanced_accuracy  \n",
      "1                 0.999900  \n",
      "3                 1.000000  \n",
      "4                 0.999801  \n",
      "0                 1.000000  \n",
      "2                 1.000000  \n",
      "\n",
      "Mean Results Across All Folds:\n",
      "fit_time                   16.120825\n",
      "score_time                  0.292781\n",
      "test_neg_log_loss          -0.378212\n",
      "train_neg_log_loss         -0.081610\n",
      "test_roc_auc                0.902557\n",
      "train_roc_auc               1.000000\n",
      "test_accuracy               0.853194\n",
      "train_accuracy              0.999971\n",
      "test_balanced_accuracy      0.773212\n",
      "train_balanced_accuracy     0.999940\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "cv_results_df_sorted = cv_results_df.sort_values(by='test_neg_log_loss', ascending=False)\n",
    "\n",
    "mean_results = cv_results_df.mean()\n",
    "print(\"Fold-level Results Sorted by Negative Log Loss:\")\n",
    "print(cv_results_df_sorted)\n",
    "\n",
    "print(\"\\nMean Results Across All Folds:\")\n",
    "print(mean_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the same performance metrics (negative log loss, ROC AUC, accuracy, and balanced accuracy) using the testing data `X_test` and `Y_test`. Display results as a dictionary.\n",
    "\n",
    "*Tip*: both, `roc_auc()` and `neg_log_loss()` will require prediction scores from `pipe.predict_proba()`. However, for `roc_auc()` you should only pass the last column `Y_pred_proba[:, 1]`. Use `Y_pred_proba` with `neg_log_loss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Recoding\n",
    "\n",
    "In the first code chunk of this document, we loaded the data and immediately recoded the target variable `income`. Why is this [convenient](https://scikit-learn.org/stable/modules/model_evaluation.html#binary-case)?\n",
    "\n",
    "The specific line was:\n",
    "\n",
    "```\n",
    "adult_dt = (pd.read_csv('../05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Answer here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_2_rubric_clean.xlsx) contains the criteria for assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-2`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_2.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Becker,Barry and Kohavi,Ronny. (1996). Adult. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
