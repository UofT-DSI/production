{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Adult* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/2/adult). Extract the data files into the subdirectory: `../05_src/data/adult/` (relative to `./05_src/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Assuming that the files `adult.data` and `adult.test` are in `../05_src/data/adult/`, then you can use the code below to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "    'native-country', 'income'\n",
    "]\n",
    "adult_dt = (pd.read_csv(r'C:\\Users\\JEFFEY MARKUS\\production\\05_src\\data\\adult\\adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data:\n",
    "\n",
    "+ Create a dataframe `X` that holds the features (all columns that are not `income`).\n",
    "+ Create a dataframe `Y` that holds the target data (`income`).\n",
    "+ From `X` and `Y`, obtain the training and testing data sets:\n",
    "\n",
    "    - Use a train-test split of 70-30%. \n",
    "    - Set the random state of the splitting function to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adult_dt.drop(columns='income')\n",
    "Y = adult_dt['income']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into test and train sets.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random States\n",
    "\n",
    "Please comment: \n",
    "\n",
    "+ What is the [random state](https://scikit-learn.org/stable/glossary.html#term-random_state) of the [splitting function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)? \n",
    "+ Why is it [useful](https://en.wikipedia.org/wiki/Reproducibility)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mRandom state is used to control the randomness of the data splitting process.\u001b[0m\n",
      "\u001b[92mBy specifying a random state, we are setting a seed for the random number generator, ensuring that the train-test split is reproducible every time we run our code.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[92mWhy is it Useful?\u001b[0m\n",
      "\n",
      "\u001b[92m1. Reproducibility: Setting a random state makes sure that if we run the same code again, we get the same split of the data into training and testing sets.\u001b[0m\n",
      "\u001b[92mThis is crucial for debugging, sharing results, and consistent model performance during experiments.\n",
      "\u001b[0m\n",
      "\u001b[92m2. Consistency in Model Comparison: When we compare different models or different configurations of the same model, it is beneficial to have a consistent train-test split.\u001b[0m\n",
      "\u001b[92mThis consistency ensures that any differences in model performance are due to the changes in the model and not variations in how the data is split.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[92mRandom state is used to control the randomness of the data splitting process.\\033[0m\")\n",
    "print(\"\\033[92mBy specifying a random state, we are setting a seed for the random number generator, ensuring that the train-test split is reproducible every time we run our code.\\n\\033[0m\")\n",
    "\n",
    "print(\"\\033[1m\\033[92mWhy is it Useful?\\033[0m\\n\")\n",
    "print(\"\\033[92m1. Reproducibility: Setting a random state makes sure that if we run the same code again, we get the same split of the data into training and testing sets.\\033[0m\")\n",
    "print(\"\\033[92mThis is crucial for debugging, sharing results, and consistent model performance during experiments.\\n\\033[0m\")\n",
    "\n",
    "print(\"\\033[92m2. Consistency in Model Comparison: When we compare different models or different configurations of the same model, it is beneficial to have a consistent train-test split.\\033[0m\")\n",
    "print(\"\\033[92mThis consistency ensures that any differences in model performance are due to the changes in the model and not variations in how the data is split.\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create a [Column Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) that treats the features as follows:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * Apply [KNN-based imputation for completing missing values](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html):\n",
    "        \n",
    "        + Consider the 7 nearest neighbours.\n",
    "        + Weight each neighbour by the inverse of its distance, causing closer neigbours to have more influence than more distant ones.\n",
    "    * [Scale features using statistics that are robust to outliers](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler).\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * Apply a [simple imputation strategy](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer):\n",
    "\n",
    "        + Use the most frequent value to complete missing values, also called the *mode*.\n",
    "\n",
    "    * Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html):\n",
    "        \n",
    "        + Handle unknown labels if they exist.\n",
    "        + Drop one column for binary variables.\n",
    "    \n",
    "    \n",
    "The column transformer should look like this:\n",
    "\n",
    "![](./images/assignment_2__column_transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "    'native-country', 'income'\n",
    "]\n",
    "data = pd.DataFrame(np.random.randn(100, len(columns)), columns=columns)  # Dummy data; replace with your data\n",
    "\n",
    "# Separating numerical and categorical columns\n",
    "numerical_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "# Define transformers for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=7, weights='distance')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='if_binary'))\n",
    "])\n",
    "\n",
    "# Creating the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Applying the preprocessing pipeline \n",
    "X_transformed = preprocessor.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `classifier` and assign a [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to it.\n",
    "\n",
    "The pipeline looks like this:\n",
    "\n",
    "![](./images/assignment_2__pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Creating the model pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),  # The ColumnTransformer from the previous step\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42))  # The classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Evaluate the model pipeline using [`cross_validate()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html):\n",
    "\n",
    "+ Measure the following [preformance metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values): negative log loss, ROC AUC, accuracy, and balanced accuracy.\n",
    "+ Report the training and validation results. \n",
    "+ Use five folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JEFFEY MARKUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\JEFFEY MARKUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Results:\n",
      "neg_log_loss - Training: -0.3118 (+/- 0.0017), Validation: -0.3281 (+/- 0.0030)\n",
      "roc_auc - Training: 0.9254 (+/- 0.0008), Validation: 0.9106 (+/- 0.0034)\n",
      "accuracy - Training: 0.8653 (+/- 0.0012), Validation: 0.8568 (+/- 0.0043)\n",
      "balanced_accuracy - Training: 0.7630 (+/- 0.0023), Validation: 0.7507 (+/- 0.0076)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, log_loss, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining the performance metrics\n",
    "scoring = {\n",
    "    'neg_log_loss': 'neg_log_loss',\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'accuracy': 'accuracy',\n",
    "    'balanced_accuracy': 'balanced_accuracy'\n",
    "}\n",
    "\n",
    "# Evaluating the model pipeline using 5-fold cross-validation\n",
    "cv_results = cross_validate(\n",
    "    model_pipeline, \n",
    "    X, \n",
    "    Y, \n",
    "    cv=5, \n",
    "    scoring=scoring, \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Reporting the training and validation results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "for metric in scoring.keys():\n",
    "    train_mean = np.mean(cv_results[f'train_{metric}'])\n",
    "    train_std = np.std(cv_results[f'train_{metric}'])\n",
    "    test_mean = np.mean(cv_results[f'test_{metric}'])\n",
    "    test_std = np.std(cv_results[f'test_{metric}'])\n",
    "    print(f\"{metric} - Training: {train_mean:.4f} (+/- {train_std:.4f}), Validation: {test_mean:.4f} (+/- {test_std:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the fold-level results as a pandas data frame and sorted by negative log loss of the test (validation) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Cross-Validation Results by Test Negative Log Loss:\n",
      "\n",
      "   fit_time  score_time  test_neg_log_loss  train_neg_log_loss  test_roc_auc  \\\n",
      "1  3.632418    0.175431          -0.331249           -0.309713      0.906679   \n",
      "0  3.129728    0.156102          -0.330795           -0.311253      0.907006   \n",
      "2  2.931507    0.144618          -0.329205           -0.313765      0.910617   \n",
      "4  3.292911    0.146331          -0.325861           -0.313799      0.914390   \n",
      "3  3.131725    0.149254          -0.323481           -0.310539      0.914494   \n",
      "\n",
      "   train_roc_auc  test_accuracy  train_accuracy  test_balanced_accuracy  \\\n",
      "1       0.926736       0.855651        0.865331                0.747288   \n",
      "0       0.925341       0.848918        0.865095                0.737764   \n",
      "2       0.924791       0.858722        0.866790                0.755843   \n",
      "4       0.924358       0.859951        0.866060                0.759700   \n",
      "3       0.925640       0.860872        0.863334                0.752904   \n",
      "\n",
      "   train_balanced_accuracy  \n",
      "1                 0.764309  \n",
      "0                 0.763306  \n",
      "2                 0.764725  \n",
      "4                 0.764245  \n",
      "3                 0.758586  \n"
     ]
    }
   ],
   "source": [
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Sorting the DataFrame by 'test_neg_log_loss'\n",
    "sorted_cv_results = cv_results_df.sort_values(by='test_neg_log_loss')\n",
    "\n",
    "# Displaying the sorted DataFrame\n",
    "print(\"Sorted Cross-Validation Results by Test Negative Log Loss:\\n\")\n",
    "print(sorted_cv_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of each metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean values for each metric across the folds:\n",
      "fit_time                   3.223658\n",
      "score_time                 0.154347\n",
      "test_neg_log_loss         -0.328118\n",
      "train_neg_log_loss        -0.311814\n",
      "test_roc_auc               0.910637\n",
      "train_roc_auc              0.925373\n",
      "test_accuracy              0.856823\n",
      "train_accuracy             0.865322\n",
      "test_balanced_accuracy     0.750700\n",
      "train_balanced_accuracy    0.763034\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "mean_metrics = cv_results_df.mean()\n",
    "\n",
    "# Displaying the mean values for each metric\n",
    "print(\"Mean values for each metric across the folds:\")\n",
    "print(mean_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the same performance metrics (negative log loss, ROC AUC, accuracy, and balanced accuracy) using the testing data `X_test` and `Y_test`. Display results as a dictionary.\n",
    "\n",
    "*Tip*: both, `roc_auc()` and `neg_log_loss()` will require prediction scores from `pipe.predict_proba()`. However, for `roc_auc()` you should only pass the last column `Y_pred_proba[:, 1]`. Use `Y_pred_proba` with `neg_log_loss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance Metrics:\n",
      "{'neg_log_loss': 0.3248655516773495, 'roc_auc': np.float64(0.9101514010597728), 'accuracy': 0.8592486436687481, 'balanced_accuracy': np.float64(0.7513226869137615)}\n"
     ]
    }
   ],
   "source": [
    "model_pipeline.fit(X_train, Y_train)\n",
    "Y_pred_proba = model_pipeline.predict_proba(X_test)\n",
    "\n",
    "# Calculating the required metrics\n",
    "test_metrics = {\n",
    "    'neg_log_loss': log_loss(Y_test, Y_pred_proba),\n",
    "    'roc_auc': roc_auc_score(Y_test, Y_pred_proba[:, 1]),\n",
    "    'accuracy': accuracy_score(Y_test, model_pipeline.predict(X_test)),\n",
    "    'balanced_accuracy': balanced_accuracy_score(Y_test, model_pipeline.predict(X_test))\n",
    "}\n",
    "\n",
    "# Displaying the test metrics as a dictionary\n",
    "print(\"Test Performance Metrics:\")\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Recoding\n",
    "\n",
    "In the first code chunk of this document, we loaded the data and immediately recoded the target variable `income`. Why is this [convenient](https://scikit-learn.org/stable/modules/model_evaluation.html#binary-case)?\n",
    "\n",
    "The specific line was:\n",
    "\n",
    "```\n",
    "adult_dt = (pd.read_csv('../05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mThe target variable 'income' was recoded to:\n",
      "  -  1 for >50K \n",
      "  -  0 for <=50K \n",
      "1.This transformation ensures compatibility with machine learning algorithms that expect numeric targets.\n",
      "2.Simplifies data processing and avoids the need for later conversions.\n",
      "3.Removes extra whitespace using 'str.strip()' for consistent data.\n",
      "4.Makes subsequent modeling steps more efficient by treating 'income' as a clean, binary numeric variable.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[92mThe target variable 'income' was recoded to:\")\n",
    "print(\"  -  1 for >50K \")\n",
    "print(\"  -  0 for <=50K \")\n",
    "print(\"1.This transformation ensures compatibility with machine learning algorithms that expect numeric targets.\")\n",
    "print(\"2.Simplifies data processing and avoids the need for later conversions.\")\n",
    "print(\"3.Removes extra whitespace using 'str.strip()' for consistent data.\")\n",
    "print(\"4.Makes subsequent modeling steps more efficient by treating 'income' as a clean, binary numeric variable.\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_2_rubric_clean.xlsx) contains the criteria for assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-2`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_2.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Becker,Barry and Kohavi,Ronny. (1996). Adult. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
