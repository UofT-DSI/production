{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "+ Feature engineering is to transform the data in such a way that the information content is easily exposed to the model.\n",
    "+ This statement can mean many things and highly depends on what exactly is \"the model\".\n",
    "+ As we have seen, we are using many tools in combination to manipulate data. Thus far, we have encountered pandas, Dask, and sklearn in this course, but there are many more (PySpark, SQL, DAX, M, R, etc.)\n",
    "+ It is important to discuss which tools are the right ones, specifically in the context of data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform using Pandas, Dask, SQL,  or Scikit-Learn?\n",
    "\n",
    "+ Most join and filtering should be done closer to the source such as a database, Spark or DataBricks.\n",
    "+ Use data manipulation tools like Pandas, Dask, or PySpark: \n",
    "    * Rename columns.\n",
    "    * Column transforms that do not require sampling.\n",
    "    * Time-series manipulation such as adding lags and contemporaneous features.\n",
    "    * Parallel computation.\n",
    "- Use ML pipelines with sklearn or PyTorch:\n",
    "    * Add features that are sample-dependent like scaling and normalization, one-hot encoding, tokenization, and vectorization.\n",
    "    * Model-dependent transformations: PCA, embeddings, iterative/knn imputation, etc.\n",
    "\n",
    "+ Decisions must be guided by optimization criteria (time and resources) while avoiding data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Transforms in sklearn\n",
    "\n",
    "The list below is from [Scikit's Documentation](https://scikit-learn.org/stable/modules/preprocessing.html), which also includes convenience interfaces for the classes listed below.\n",
    "\n",
    "Work with categorical variables:\n",
    "\n",
    "+ `preprocessing.Binarizer(*[, threshold, copy])`: Binarize data (set feature values to 0 or 1) according to a threshold.\n",
    "+ `preprocessing.KBinsDiscretizer([n_bins, ...])`:  Bin continuous data into intervals.\n",
    "+ `preprocessing.LabelBinarizer(*[, neg_label, ...])`: Binarize labels in a one-vs-all fashion.\n",
    "+ `preprocessing.LabelEncoder()`: Encode target labels with value between 0 and n_classes-1.\n",
    "+ `preprocessing.MultiLabelBinarizer(*[, ...])`:  Transform between iterable of iterables and a multilabel format.\n",
    "+ `preprocessing.OneHotEncoder(*[, categories, ...])`: Encode categorical features as a one-hot numeric array.\n",
    "+ `preprocessing.OrdinalEncoder(*[, ...])`: Encode categorical features as an integer array.\n",
    "\n",
    "Scale and normalize:\n",
    "\n",
    "+ `preprocessing.StandardScaler(*[, copy, ...])`: Standardize features by removing the mean and scaling to unit variance.\n",
    "+ `preprocessing.MaxAbsScaler(*[, copy])`: Scale each feature by its maximum absolute value.\n",
    "+ `preprocessing.MinMaxScaler([feature_range, ...])`: Transform features by scaling each feature to a given range.\n",
    "+ `preprocessing.Normalizer([norm, copy])`:  Normalize samples individually to unit norm.\n",
    "+ `preprocessing.RobustScaler(*[, ...])`: Scale features using statistics that are robust to outliers.\n",
    "\n",
    "\n",
    "Nonlinear transforms:\n",
    "\n",
    "+ `preprocessing.FunctionTransformer([func, ...])`: Constructs a transformer from an arbitrary callable.\n",
    "+ `preprocessing.KernelCenterer()`: Center an arbitrary kernel matrix \n",
    "+ `preprocessing.PolynomialFeatures([degree, ...])`: Generate polynomial and interaction features.\n",
    "+ `preprocessing.PowerTransformer([method, ...])`: Apply a power transform featurewise to make data more Gaussian-like.\n",
    "+ `preprocessing.QuantileTransformer(*[, ...])`: Transform features using quantiles information.\n",
    "+ `preprocessing.SplineTransformer([n_knots, ...])`: Generate univariate B-spline bases for features.\n",
    "+ `preprocessing.TargetEncoder([categories, ...])`: Target Encoder for regression and classification targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we doing?\n",
    "\n",
    "<div>\n",
    "<img src=\"./images/04_column_transform_1.png\" width=\"75%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Objectives\n",
    "\n",
    "Build a pipeline that: \n",
    "\n",
    "+ Add indicators: \n",
    "\n",
    "    - SME indicated that a Debt-to-Ratio > 100% is too high.\n",
    "    - Missing values indicator for `monthly_income` and `num_dependents`.\n",
    "\n",
    "+ Impute missing values, where required.\n",
    "+ Standardize variables.\n",
    "+ Evaluate if a transform (Yeo-Johnson or Box-Cox) of selected variables (debt_ratio, monthly_income, and revolving_unsecured_line_utilization) is beneficial.\n",
    "\n",
    "Feature selection:\n",
    "\n",
    "+ We are looking for informative features: their contribution to prediction is valuable.\n",
    "+ We prefer parsimonious models.\n",
    "+ We want to retain evidence of our work and ensure reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "\n",
    "+ For this example, we will use [Give Me Some Credit from Kaggle](https://www.kaggle.com/c/GiveMeSomeCredit/data), a widely refered example. \n",
    "+ To run the examples below, download the data set and extract cs-training.csv to `../05_src/data/credit/`.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv \n",
    "%run update_path.py\n",
    "\n",
    "import os\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load data\n",
    "ft_file = os.getenv(\"CREDIT_DATA\")\n",
    "df_raw = pd.read_csv(ft_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.drop(columns = [\"Unnamed: 0\"]).rename(\n",
    "    columns = {\n",
    "        'SeriousDlqin2yrs': 'delinquency',\n",
    "        'RevolvingUtilizationOfUnsecuredLines': 'revolving_unsecured_line_utilization', \n",
    "        'age': 'age',\n",
    "        'NumberOfTime30-59DaysPastDueNotWorse': 'num_30_59_days_late', \n",
    "        'DebtRatio': 'debt_ratio', \n",
    "        'MonthlyIncome': 'monthly_income',\n",
    "        'NumberOfOpenCreditLinesAndLoans': 'num_open_credit_loans', \n",
    "        'NumberOfTimes90DaysLate':  'num_90_days_late',\n",
    "        'NumberRealEstateLoansOrLines': 'num_real_estate_loans', \n",
    "        'NumberOfTime60-89DaysPastDueNotWorse': 'num_60_89_days_late',\n",
    "        'NumberOfDependents': 'num_dependents'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Solution\n",
    "\n",
    "To get some insights into the task, first approach it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_cols = ['revolving_unsecured_line_utilization', 'age',\n",
    "       'num_30_59_days_late', 'debt_ratio', 'monthly_income',\n",
    "       'num_open_credit_loans', 'num_90_days_late', 'num_real_estate_loans',\n",
    "       'num_60_89_days_late', 'num_dependents'\n",
    "       ]\n",
    "\n",
    "pipe_num_simple = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('standardizer', StandardScaler())\n",
    "])\n",
    "\n",
    "ctransform_simple= ColumnTransformer([\n",
    "    ('numeric_simple', pipe_num_simple, num_cols),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipe_simple = Pipeline([\n",
    "    ('preprocess', ctransform_simple),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "pipe_simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation of Simple Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'delinquency')\n",
    "Y = df['delinquency']\n",
    "\n",
    "scoring = ['neg_log_loss', 'roc_auc', 'f1', 'accuracy', 'precision', 'recall']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_simple_dict = cross_validate(pipe_simple, X_train, Y_train, cv = 5, scoring = scoring)\n",
    "res_simple = pd.DataFrame(res_simple_dict).assign(experiment = 1)\n",
    "res_simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, we obtain a log-loss of about 0.23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_simple.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Pipeline\n",
    "\n",
    "- The pipeline below is more complex.\n",
    "- Treat selected numericals using [Yeo-Johnson transformation](https://feature-engine.trainindata.com/en/latest/user_guide/transformation/YeoJohnsonTransformer.html).\n",
    "- Treat other numericals with scaling only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['age',\n",
    "       'num_30_59_days_late', 'num_open_credit_loans', 'num_90_days_late', 'num_real_estate_loans',\n",
    "       'num_60_89_days_late', 'num_dependents', \n",
    "       ]\n",
    "\n",
    "num_cols_transform = ['revolving_unsecured_line_utilization', 'debt_ratio', 'monthly_income',]\n",
    "\n",
    "pipe_num_simple = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('standardizer', StandardScaler())\n",
    "])\n",
    "\n",
    "pipe_num_yj = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('standardizer', StandardScaler()),\n",
    "    ('transform', PowerTransformer(method='yeo-johnson'))\n",
    "])\n",
    "\n",
    "ctramsform_yj = ColumnTransformer([\n",
    "    ('numeric_std', pipe_num_simple, num_cols),\n",
    "    ('numeric_yj', pipe_num_yj, num_cols_transform),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipe_yj = Pipeline([\n",
    "    ('preprocess', ctramsform_yj),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "pipe_yj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_yj_dict = cross_validate(pipe_yj, X_train, Y_train, cv = 5, scoring = scoring)\n",
    "res_yj = pd.DataFrame(res_yj_dict).assign(experiment = 2)\n",
    "res_yj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a loss of 0.22, therefore the additional feature enhances performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_yj.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "+ We are currently evaluating two feature engineering procedures using the same classifier. \n",
    "\n",
    "    - However, feature engineering is classifier-dependent: each classifier is a specialized tool to learn a certain type of hypothesis. \n",
    "    - Different classifiers will benefit from different types of engineered features (see, for example, [Khun and Silge's recommendations on TMWR.org](https://www.tmwr.org/pre-proc-table)).\n",
    "\n",
    "+ We are producing data from our experiments.\n",
    "\n",
    "    - The data that we produced is more or less structured: we are using standard performance metrics, for instance.\n",
    "    - Each preprocessing pipeline will be different and may accept different configuration parameters.\n",
    "    - Likewise, classifiers will tend to have different configuration parameters. \n",
    "    \n",
    "+ We modify code to produce experiments:\n",
    "\n",
    "    - Our experiment results will be a function of our algorithm's logic, its implementation (code), and our data.\n",
    "    - Code tracking is done with Git.\n",
    "    - Data tracking is in development.\n",
    "\n",
    "**It is generally a good idea to use software for experiment tracking once you move out of the Proof of Concept stage.** Some solutions include:\n",
    "\n",
    "- [ML Flow](https://mlflow.org/).\n",
    "- [Weights & Balances](https://wandb.ai/site).\n",
    "- [Sacred](https://sacred.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow\n",
    "\n",
    "+ MLFlow is a software tool that automates tasks related to experiment tracking:\n",
    "\n",
    "    - Keep track of experiment parameters.\n",
    "    - Save configurations for individual experiment runs in files or databases.\n",
    "    - Store models and other artifacts in an object store.\n",
    "\n",
    "+ A few features that may be useful:\n",
    "\n",
    "    - Keep track of code and artifacts associated with the experiment.\n",
    "    - Store experiment run times and system characteristics.\n",
    "    - Work with different backend stores (\"[Observers](https://mlflow.org/docs/latest/tracking/backend-stores)\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Experiment\n",
    "\n",
    "Continuing with our example, the following setup will track an experiment to measure the performance of a model pipeline. The main file for this experiment is `./05_src/credit/exp__logistic_simple.py`. You can run this experiment from the `05_src/` folder using `python -m credit.exp__logistic_simple`.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the experiment, take a look at MLFlow by navigating to [http://localhost:5001](http://localhost:5001).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "production-env (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
