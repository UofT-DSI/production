{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are we doing?\n",
    "\n",
    "## Objectives \n",
    "\n",
    "\n",
    "* Build a data pipeline that downloads price data from the internet, stores it locally, transforms it into return data, and stores the feature set.\n",
    "    - Getting the data.\n",
    "    - Schemas and index in Dask.\n",
    "\n",
    "* Explore the parquet format.\n",
    "    - Reading and writing parquet files.\n",
    "    - Read datasets that are stored in distributed files.\n",
    "    - Discuss Dask vs. Pandas as a small example of big vs small data.\n",
    "    \n",
    "* Discuss the use of environment variables for settings.\n",
    "* Discuss how to use Jupyter notebooks and source code concurrently. \n",
    "* Logging and using a standard logger.\n",
    "\n",
    "## About the Data\n",
    "\n",
    "+ We will download the prices for a list of stocks.\n",
    "+ The source is Yahoo Finance, and the data, along with its description, is available via [Kaggle](https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset).\n",
    "\n",
    "\n",
    "## Medallion Architecture\n",
    "\n",
    "+ The architecture that we are thinking about is called Medallion by [DataBricks](https://www.databricks.com/glossary/medallion-architecture). It is an ELT type of thinking, although our data is well-structured.\n",
    "\n",
    "<div>\n",
    "<image src=\"./images/02_medallion_architecture.png\" height=300>\n",
    "</div>\n",
    "\n",
    "+ In our case, we would like to optimize the number of times that we download data from the internet. \n",
    "+ Ultimately, we will build a pipeline manager class to control the process of obtaining and transforming our data.\n",
    "\n",
    "<div>\n",
    "<image src=\"./images/02_target_pipeline_manager.png\" height=250>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "\n",
    "Download the [Stock Market Dataset from Kaggle](https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset). Note that you may be required to register for a free account. Alternatively, download the file from [this location](https://drive.google.com/drive/folders/1AA4gapDLpI194TGce1bY25sd91Km-tU3?usp=drive_link).\n",
    "\n",
    "+ Extract stock prices (not ETFs) into the directory: `./05_src/data/prices_csv/`. \n",
    "+ To be clear, your folder structure should include the path `05_src/data/prices_csv/stocks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command `%run update_path.py` runs a local script that adds the repository's `./05_src/` directory to the Notebook's kernel path. This way, we can use our modules within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run update_path.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the historical price data for stocks and ETFs, use the code below. Notice the following:\n",
    "\n",
    "+ Libraries are ordered from high-level to low-level libraries from the package manager. Local modules are imported at the end. \n",
    "+ The function `get_logger()` is called with `__name__` as recommended by [Python's documentation](https://docs.python.org/2/howto/logging.html#logging-advanced-tutorial).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from utils.logger import get_logger\n",
    "_logs = get_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The [`glob` module](https://docs.python.org/3/library/glob.html) is used for finding path names that match specified patterns using Unix shell-style rules.\n",
    "\n",
    "+ Notice that the module `glob` contains a function called `glob`; therefore, we used `from glob import glob` above.\n",
    "\n",
    "+ The path in which we are searching for our csv files is produced by joining two strings:\n",
    "\n",
    "\n",
    "\n",
    "    - The value of the environment variable 'SRC_DIR' that we obtain with `os.getenv('SRC_DIR')` (this variable points to ./05_src).\n",
    "\n",
    "    - Another string given by \"data/prices_csv/stocks/*.csv\".\n",
    "\n",
    "    - Both strings are combined into an OS-consistent path using `os.path.join(...)`.\n",
    "\n",
    "+ After we know the location of all our files, we sample a subset of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 12:12:27,444, 3619788285.py, 2, INFO, Found 5884 stock price files.\n",
      "2026-01-16 12:12:27,455, 3619788285.py, 7, INFO, Sampled 60 stock price files for processing. The files are: ['../../05_src/data/prices_csv/stocks\\\\TNC.csv', '../../05_src/data/prices_csv/stocks\\\\CBB.csv', '../../05_src/data/prices_csv/stocks\\\\ALDX.csv', '../../05_src/data/prices_csv/stocks\\\\GLADD.csv', '../../05_src/data/prices_csv/stocks\\\\FIXX.csv', '../../05_src/data/prices_csv/stocks\\\\ETJ.csv', '../../05_src/data/prices_csv/stocks\\\\CMCTP.csv', '../../05_src/data/prices_csv/stocks\\\\BWG.csv', '../../05_src/data/prices_csv/stocks\\\\VIAC.csv', '../../05_src/data/prices_csv/stocks\\\\REI.csv', '../../05_src/data/prices_csv/stocks\\\\BLPH.csv', '../../05_src/data/prices_csv/stocks\\\\SMG.csv', '../../05_src/data/prices_csv/stocks\\\\MOH.csv', '../../05_src/data/prices_csv/stocks\\\\AMH.csv', '../../05_src/data/prices_csv/stocks\\\\AMAL.csv', '../../05_src/data/prices_csv/stocks\\\\BPYPN.csv', '../../05_src/data/prices_csv/stocks\\\\ERH.csv', '../../05_src/data/prices_csv/stocks\\\\FAMI.csv', '../../05_src/data/prices_csv/stocks\\\\PFG.csv', '../../05_src/data/prices_csv/stocks\\\\SPXC.csv', '../../05_src/data/prices_csv/stocks\\\\ALL.csv', '../../05_src/data/prices_csv/stocks\\\\RTTR.csv', '../../05_src/data/prices_csv/stocks\\\\EARN.csv', '../../05_src/data/prices_csv/stocks\\\\ZIXI.csv', '../../05_src/data/prices_csv/stocks\\\\TSN.csv', '../../05_src/data/prices_csv/stocks\\\\WST.csv', '../../05_src/data/prices_csv/stocks\\\\REG.csv', '../../05_src/data/prices_csv/stocks\\\\MNK.csv', '../../05_src/data/prices_csv/stocks\\\\ESGR.csv', '../../05_src/data/prices_csv/stocks\\\\NGD.csv', '../../05_src/data/prices_csv/stocks\\\\SLRX.csv', '../../05_src/data/prices_csv/stocks\\\\GLW.csv', '../../05_src/data/prices_csv/stocks\\\\ACN.csv', '../../05_src/data/prices_csv/stocks\\\\CSSE.csv', '../../05_src/data/prices_csv/stocks\\\\WORK.csv', '../../05_src/data/prices_csv/stocks\\\\MOS.csv', '../../05_src/data/prices_csv/stocks\\\\IPWR.csv', '../../05_src/data/prices_csv/stocks\\\\GLUU.csv', '../../05_src/data/prices_csv/stocks\\\\CRMT.csv', '../../05_src/data/prices_csv/stocks\\\\EOLS.csv', '../../05_src/data/prices_csv/stocks\\\\INSU.csv', '../../05_src/data/prices_csv/stocks\\\\BWEN.csv', '../../05_src/data/prices_csv/stocks\\\\BPMX.csv', '../../05_src/data/prices_csv/stocks\\\\LH.csv', '../../05_src/data/prices_csv/stocks\\\\BRQS.csv', '../../05_src/data/prices_csv/stocks\\\\KALU.csv', '../../05_src/data/prices_csv/stocks\\\\ITCB.csv', '../../05_src/data/prices_csv/stocks\\\\SRE.csv', '../../05_src/data/prices_csv/stocks\\\\GAZ.csv', '../../05_src/data/prices_csv/stocks\\\\AQMS.csv', '../../05_src/data/prices_csv/stocks\\\\NPK.csv', '../../05_src/data/prices_csv/stocks\\\\QRHC.csv', '../../05_src/data/prices_csv/stocks\\\\CGEN.csv', '../../05_src/data/prices_csv/stocks\\\\LEVL.csv', '../../05_src/data/prices_csv/stocks\\\\BGS.csv', '../../05_src/data/prices_csv/stocks\\\\RIV.csv', '../../05_src/data/prices_csv/stocks\\\\GURE.csv', '../../05_src/data/prices_csv/stocks\\\\TEF.csv', '../../05_src/data/prices_csv/stocks\\\\SYNH.csv', '../../05_src/data/prices_csv/stocks\\\\KEY.csv']\n"
     ]
    }
   ],
   "source": [
    "stock_files = glob(os.path.join(os.getenv('SRC_DIR'), \"data/prices_csv/stocks/*.csv\"))\n",
    "_logs.info(f'Found {len(stock_files)} stock price files.')\n",
    "\n",
    "random.seed(42)\n",
    "n_sample = 60\n",
    "stock_files = random.sample(stock_files, n_sample)\n",
    "_logs.info(f'Sampled {n_sample} stock price files for processing. The files are: {stock_files   }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the sampled files into dataframes and concatenate them:\n",
    "\n",
    "+ Start with an empty list.\n",
    "+ Read each file into a dataframe and [`append()` it to the list](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists). Notice that `append()` is an in-place operation (it does not return a list, it modifies the list in place).\n",
    "+ Finally, we concatenate all dataframes along the vertical axis (`axis=0`) using [`pd.concat()`](https://pandas.pydata.org/docs/user_guide/merging.html#concat). \n",
    "+ Notice that we do not concatenate each time that we load a dataframe. According to [Panda's documentation](https://pandas.pydata.org/docs/user_guide/merging.html#concat): \n",
    "\n",
    "> \"`concat()` makes a full copy of the data, and iteratively reusing `concat()` can create unnecessary copies. Collect all DataFrame or Series objects in a list before using `concat()`.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 13:34:17,038, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\TNC.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 13:34:17,132, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\CBB.csv\n",
      "2026-01-16 13:34:17,206, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\ALDX.csv\n",
      "2026-01-16 13:34:17,255, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\GLADD.csv\n",
      "2026-01-16 13:34:17,271, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\FIXX.csv\n",
      "2026-01-16 13:34:17,284, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\ETJ.csv\n",
      "2026-01-16 13:34:17,425, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\CMCTP.csv\n",
      "2026-01-16 13:34:17,443, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\BWG.csv\n",
      "2026-01-16 13:34:17,500, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\VIAC.csv\n",
      "2026-01-16 13:34:17,519, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\REI.csv\n",
      "2026-01-16 13:34:17,542, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\BLPH.csv\n",
      "2026-01-16 13:34:17,579, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\SMG.csv\n",
      "2026-01-16 13:34:17,618, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\MOH.csv\n",
      "2026-01-16 13:34:17,665, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\AMH.csv\n",
      "2026-01-16 13:34:17,689, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\AMAL.csv\n",
      "2026-01-16 13:34:17,703, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\BPYPN.csv\n",
      "2026-01-16 13:34:17,712, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\ERH.csv\n",
      "2026-01-16 13:34:17,760, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\FAMI.csv\n",
      "2026-01-16 13:34:17,775, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\PFG.csv\n",
      "2026-01-16 13:34:17,825, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\SPXC.csv\n",
      "2026-01-16 13:34:17,860, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\ALL.csv\n",
      "2026-01-16 13:34:17,943, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\RTTR.csv\n",
      "2026-01-16 13:34:17,975, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\EARN.csv\n",
      "2026-01-16 13:34:17,991, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\ZIXI.csv\n",
      "2026-01-16 13:34:18,043, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\TSN.csv\n",
      "2026-01-16 13:34:18,081, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\WST.csv\n",
      "2026-01-16 13:34:18,136, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\REG.csv\n",
      "2026-01-16 13:34:18,168, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\MNK.csv\n",
      "2026-01-16 13:34:18,202, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\ESGR.csv\n",
      "2026-01-16 13:34:18,230, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\NGD.csv\n",
      "2026-01-16 13:34:18,274, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\SLRX.csv\n",
      "2026-01-16 13:34:18,293, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\GLW.csv\n",
      "2026-01-16 13:34:18,359, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\ACN.csv\n",
      "2026-01-16 13:34:18,410, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\CSSE.csv\n",
      "2026-01-16 13:34:18,429, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\WORK.csv\n",
      "2026-01-16 13:34:18,445, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\MOS.csv\n",
      "2026-01-16 13:34:18,508, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\IPWR.csv\n",
      "2026-01-16 13:34:18,523, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\GLUU.csv\n",
      "2026-01-16 13:34:18,575, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\CRMT.csv\n",
      "2026-01-16 13:34:18,614, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\EOLS.csv\n",
      "2026-01-16 13:34:18,645, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\INSU.csv\n",
      "2026-01-16 13:34:18,658, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\BWEN.csv\n",
      "2026-01-16 13:34:18,690, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\BPMX.csv\n",
      "2026-01-16 13:34:18,725, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\LH.csv\n",
      "2026-01-16 13:34:18,762, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\BRQS.csv\n",
      "2026-01-16 13:34:18,818, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\KALU.csv\n",
      "2026-01-16 13:34:18,843, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\ITCB.csv\n",
      "2026-01-16 13:34:18,888, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\SRE.csv\n",
      "2026-01-16 13:34:18,935, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\GAZ.csv\n",
      "2026-01-16 13:34:18,967, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\AQMS.csv\n",
      "2026-01-16 13:34:18,985, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\NPK.csv\n",
      "2026-01-16 13:34:19,041, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\QRHC.csv\n",
      "2026-01-16 13:34:19,090, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\CGEN.csv\n",
      "2026-01-16 13:34:19,119, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\LEVL.csv\n",
      "2026-01-16 13:34:19,133, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\BGS.csv\n",
      "2026-01-16 13:34:19,195, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\RIV.csv\n",
      "2026-01-16 13:34:19,214, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\GURE.csv\n",
      "2026-01-16 13:34:19,281, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\TEF.csv\n",
      "2026-01-16 13:34:19,349, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\SYNH.csv\n",
      "2026-01-16 13:34:19,362, 2894238413.py, 3, INFO, Reading file: ../../05_src/data/prices_csv/stocks\\KEY.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 239659 entries, 0 to 239658\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   Date       239659 non-null  datetime64[ns]\n",
      " 1   Open       239656 non-null  float64       \n",
      " 2   High       239656 non-null  float64       \n",
      " 3   Low        239656 non-null  float64       \n",
      " 4   Close      239656 non-null  float64       \n",
      " 5   Adj Close  239656 non-null  float64       \n",
      " 6   Volume     239656 non-null  float64       \n",
      " 7   source     239659 non-null  object        \n",
      " 8   ticker     239659 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(6), object(2)\n",
      "memory usage: 16.5+ MB\n"
     ]
    }
   ],
   "source": [
    "dt_list = []\n",
    "for s_file in stock_files:\n",
    "    _logs.info(f\"Reading file: {s_file}\")\n",
    "    dt = pd.read_csv(s_file).assign(\n",
    "        source = os.path.basename(s_file),\n",
    "        ticker = os.path.basename(s_file).replace('.csv', ''),\n",
    "        Date = lambda x: pd.to_datetime(x['Date'])\n",
    "    )\n",
    "    dt_list.append(dt)\n",
    "stock_prices = pd.concat(dt_list, axis = 0, ignore_index = True)\n",
    "stock_prices.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the structure of the `stock_prices` data using the [`info()` dataframe method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 239659 entries, 0 to 239658\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   Date       239659 non-null  datetime64[ns]\n",
      " 1   Open       239656 non-null  float64       \n",
      " 2   High       239656 non-null  float64       \n",
      " 3   Low        239656 non-null  float64       \n",
      " 4   Close      239656 non-null  float64       \n",
      " 5   Adj Close  239656 non-null  float64       \n",
      " 6   Volume     239656 non-null  float64       \n",
      " 7   source     239659 non-null  object        \n",
      " 8   ticker     239659 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(6), object(2)\n",
      "memory usage: 16.5+ MB\n"
     ]
    }
   ],
   "source": [
    "dt_list = []\n",
    "for s_file in stock_files[:2]:\n",
    "    \n",
    "    dt = pd.read_csv(s_file).assign(\n",
    "        source = os.path.basename(s_file),\n",
    "        ticker = os.path.basename(s_file).replace('.csv', ''),\n",
    "        Date = lambda x: pd.to_datetime(x['Date'])\n",
    "    )\n",
    "    dt_list.append(dt)\n",
    "\n",
    "stock_prices = pd.concat(dt_list, axis = 0, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21981 entries, 0 to 21980\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Date       21981 non-null  datetime64[ns]\n",
      " 1   Open       21981 non-null  float64       \n",
      " 2   High       21981 non-null  float64       \n",
      " 3   Low        21981 non-null  float64       \n",
      " 4   Close      21981 non-null  float64       \n",
      " 5   Adj Close  21981 non-null  float64       \n",
      " 6   Volume     21981 non-null  int64         \n",
      " 7   source     21981 non-null  object        \n",
      " 8   ticker     21981 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(5), int64(1), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "stock_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>source</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1973-02-22</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.507389</td>\n",
       "      <td>800</td>\n",
       "      <td>TNC.csv</td>\n",
       "      <td>TNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1973-02-23</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.507389</td>\n",
       "      <td>800</td>\n",
       "      <td>TNC.csv</td>\n",
       "      <td>TNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1973-02-26</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.507389</td>\n",
       "      <td>800</td>\n",
       "      <td>TNC.csv</td>\n",
       "      <td>TNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1973-02-27</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.507389</td>\n",
       "      <td>0</td>\n",
       "      <td>TNC.csv</td>\n",
       "      <td>TNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1973-02-28</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.507389</td>\n",
       "      <td>0</td>\n",
       "      <td>TNC.csv</td>\n",
       "      <td>TNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21976</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>14.58</td>\n",
       "      <td>14.84</td>\n",
       "      <td>14.38</td>\n",
       "      <td>14.66</td>\n",
       "      <td>14.660000</td>\n",
       "      <td>643500</td>\n",
       "      <td>CBB.csv</td>\n",
       "      <td>CBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21977</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>14.50</td>\n",
       "      <td>14.82</td>\n",
       "      <td>14.50</td>\n",
       "      <td>14.73</td>\n",
       "      <td>14.730000</td>\n",
       "      <td>627000</td>\n",
       "      <td>CBB.csv</td>\n",
       "      <td>CBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21978</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>14.76</td>\n",
       "      <td>14.84</td>\n",
       "      <td>14.49</td>\n",
       "      <td>14.70</td>\n",
       "      <td>14.700000</td>\n",
       "      <td>308300</td>\n",
       "      <td>CBB.csv</td>\n",
       "      <td>CBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21979</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>14.62</td>\n",
       "      <td>14.85</td>\n",
       "      <td>14.60</td>\n",
       "      <td>14.64</td>\n",
       "      <td>14.640000</td>\n",
       "      <td>684200</td>\n",
       "      <td>CBB.csv</td>\n",
       "      <td>CBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21980</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>14.50</td>\n",
       "      <td>14.66</td>\n",
       "      <td>14.39</td>\n",
       "      <td>14.61</td>\n",
       "      <td>14.610000</td>\n",
       "      <td>642600</td>\n",
       "      <td>CBB.csv</td>\n",
       "      <td>CBB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21981 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date   Open   High    Low  Close  Adj Close  Volume   source  \\\n",
       "0     1973-02-22   4.25   4.25   4.25   4.25   0.507389     800  TNC.csv   \n",
       "1     1973-02-23   4.25   4.25   4.25   4.25   0.507389     800  TNC.csv   \n",
       "2     1973-02-26   4.25   4.25   4.25   4.25   0.507389     800  TNC.csv   \n",
       "3     1973-02-27   4.25   4.25   4.25   4.25   0.507389       0  TNC.csv   \n",
       "4     1973-02-28   4.25   4.25   4.25   4.25   0.507389       0  TNC.csv   \n",
       "...          ...    ...    ...    ...    ...        ...     ...      ...   \n",
       "21976 2020-03-26  14.58  14.84  14.38  14.66  14.660000  643500  CBB.csv   \n",
       "21977 2020-03-27  14.50  14.82  14.50  14.73  14.730000  627000  CBB.csv   \n",
       "21978 2020-03-30  14.76  14.84  14.49  14.70  14.700000  308300  CBB.csv   \n",
       "21979 2020-03-31  14.62  14.85  14.60  14.64  14.640000  684200  CBB.csv   \n",
       "21980 2020-04-01  14.50  14.66  14.39  14.61  14.610000  642600  CBB.csv   \n",
       "\n",
       "      ticker  \n",
       "0        TNC  \n",
       "1        TNC  \n",
       "2        TNC  \n",
       "3        TNC  \n",
       "4        TNC  \n",
       "...      ...  \n",
       "21976    CBB  \n",
       "21977    CBB  \n",
       "21978    CBB  \n",
       "21979    CBB  \n",
       "21980    CBB  \n",
       "\n",
       "[21981 rows x 9 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stock_prices = pd.concat(dt_list, axis = 0, ignore_index = True)\n",
    "stock_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 239659 entries, 0 to 239658\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   Date       239659 non-null  datetime64[ns]\n",
      " 1   Open       239656 non-null  float64       \n",
      " 2   High       239656 non-null  float64       \n",
      " 3   Low        239656 non-null  float64       \n",
      " 4   Close      239656 non-null  float64       \n",
      " 5   Adj Close  239656 non-null  float64       \n",
      " 6   Volume     239656 non-null  float64       \n",
      " 7   source     239659 non-null  object        \n",
      " 8   ticker     239659 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(6), object(2)\n",
      "memory usage: 16.5+ MB\n"
     ]
    }
   ],
   "source": [
    "stock_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TNC', 'CBB'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_prices['ticker'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>source</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987-11-05</td>\n",
       "      <td>6.154910</td>\n",
       "      <td>6.189488</td>\n",
       "      <td>6.085754</td>\n",
       "      <td>6.120332</td>\n",
       "      <td>0.433743</td>\n",
       "      <td>55000</td>\n",
       "      <td>KEY.csv</td>\n",
       "      <td>KEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987-11-06</td>\n",
       "      <td>6.154910</td>\n",
       "      <td>6.224066</td>\n",
       "      <td>6.120332</td>\n",
       "      <td>6.120332</td>\n",
       "      <td>0.433743</td>\n",
       "      <td>212000</td>\n",
       "      <td>KEY.csv</td>\n",
       "      <td>KEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987-11-09</td>\n",
       "      <td>6.085754</td>\n",
       "      <td>6.154910</td>\n",
       "      <td>5.947441</td>\n",
       "      <td>5.947441</td>\n",
       "      <td>0.421490</td>\n",
       "      <td>107600</td>\n",
       "      <td>KEY.csv</td>\n",
       "      <td>KEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1987-11-10</td>\n",
       "      <td>6.016598</td>\n",
       "      <td>6.154910</td>\n",
       "      <td>5.947441</td>\n",
       "      <td>6.120332</td>\n",
       "      <td>0.433743</td>\n",
       "      <td>61600</td>\n",
       "      <td>KEY.csv</td>\n",
       "      <td>KEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987-11-11</td>\n",
       "      <td>6.016598</td>\n",
       "      <td>6.189488</td>\n",
       "      <td>6.016598</td>\n",
       "      <td>6.120332</td>\n",
       "      <td>0.433743</td>\n",
       "      <td>65600</td>\n",
       "      <td>KEY.csv</td>\n",
       "      <td>KEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8161</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>9.970000</td>\n",
       "      <td>11.590000</td>\n",
       "      <td>9.650000</td>\n",
       "      <td>11.460000</td>\n",
       "      <td>11.460000</td>\n",
       "      <td>22494500</td>\n",
       "      <td>KEY.csv</td>\n",
       "      <td>KEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8162</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>10.720000</td>\n",
       "      <td>11.690000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>20794800</td>\n",
       "      <td>KEY.csv</td>\n",
       "      <td>KEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8163</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>11.040000</td>\n",
       "      <td>11.240000</td>\n",
       "      <td>10.380000</td>\n",
       "      <td>10.790000</td>\n",
       "      <td>10.790000</td>\n",
       "      <td>15175400</td>\n",
       "      <td>KEY.csv</td>\n",
       "      <td>KEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8164</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>10.680000</td>\n",
       "      <td>10.860000</td>\n",
       "      <td>10.120000</td>\n",
       "      <td>10.370000</td>\n",
       "      <td>10.370000</td>\n",
       "      <td>15997000</td>\n",
       "      <td>KEY.csv</td>\n",
       "      <td>KEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8165</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>9.630000</td>\n",
       "      <td>9.720000</td>\n",
       "      <td>9.350000</td>\n",
       "      <td>9.450000</td>\n",
       "      <td>9.450000</td>\n",
       "      <td>14533700</td>\n",
       "      <td>KEY.csv</td>\n",
       "      <td>KEY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8166 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date       Open       High        Low      Close  Adj Close  \\\n",
       "0    1987-11-05   6.154910   6.189488   6.085754   6.120332   0.433743   \n",
       "1    1987-11-06   6.154910   6.224066   6.120332   6.120332   0.433743   \n",
       "2    1987-11-09   6.085754   6.154910   5.947441   5.947441   0.421490   \n",
       "3    1987-11-10   6.016598   6.154910   5.947441   6.120332   0.433743   \n",
       "4    1987-11-11   6.016598   6.189488   6.016598   6.120332   0.433743   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "8161 2020-03-26   9.970000  11.590000   9.650000  11.460000  11.460000   \n",
       "8162 2020-03-27  10.720000  11.690000  10.700000  11.200000  11.200000   \n",
       "8163 2020-03-30  11.040000  11.240000  10.380000  10.790000  10.790000   \n",
       "8164 2020-03-31  10.680000  10.860000  10.120000  10.370000  10.370000   \n",
       "8165 2020-04-01   9.630000   9.720000   9.350000   9.450000   9.450000   \n",
       "\n",
       "        Volume   source ticker  \n",
       "0        55000  KEY.csv    KEY  \n",
       "1       212000  KEY.csv    KEY  \n",
       "2       107600  KEY.csv    KEY  \n",
       "3        61600  KEY.csv    KEY  \n",
       "4        65600  KEY.csv    KEY  \n",
       "...        ...      ...    ...  \n",
       "8161  22494500  KEY.csv    KEY  \n",
       "8162  20794800  KEY.csv    KEY  \n",
       "8163  15175400  KEY.csv    KEY  \n",
       "8164  15997000  KEY.csv    KEY  \n",
       "8165  14533700  KEY.csv    KEY  \n",
       "\n",
       "[8166 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subset our ticker data set using standard indexing techniques. Good references for this type of data manipulation are:\n",
    "\n",
    "+ [Panda's Documentation](https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-and-selecting-data). \n",
    "+ [Panda's Cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html#cookbook-selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the subset data frame, select one column and convert to list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TNC', 'CBB']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_tickers = stock_prices['ticker'].unique().tolist()\n",
    "select_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Data in CSV\n",
    "\n",
    "+ We have some data. How do we store it?\n",
    "+ We can compare two options, CSV and Parquet, by measuring their performance:\n",
    "\n",
    "    - Time to save: We will measure time by using the `time` library.\n",
    "    - Space required on drive: We will use the custom function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir_size(path='.'):\n",
    "    '''Returns the total size of files contained in path.'''\n",
    "    total = 0\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "            elif entry.is_dir():\n",
    "                total += get_dir_size(entry.path)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the specification of a temporary directory from the environment  and create a subdirectory in it called \"csv\":\n",
    "\n",
    "+ Use `os.getenv(\"TEMP_DATA\")` to obtain the desired location of the temporary folder from an environment variable.\n",
    "+ If the subdirectory exists, delete it using `shutil.rmtree()`; the flag `ignore_errors=True` helps us in case the subdirectory does not exist (for instance, in the first run).\n",
    "+ Create a directory with path given by `csv_dir` using `os.makedirs()`; the flag `exist_ok=True` indicates that if the directory already exists, then the function will do nothing.\n",
    "+ Finally, create the stock price file location, `stock_csv`, which will be used to create the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "temp = os.getenv(\"TEMP_DATA\")\n",
    "csv_dir = os.path.join(temp, \"csv\")\n",
    "\n",
    "shutil.rmtree(csv_dir, ignore_errors=True)\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "stock_csv = os.path.join(csv_dir, \"stock_px.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the concatenated dataframe to a CSV file. We measure the time elapsed by storing the start and end times, then we calculate their difference in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 12:53:13,184, 807204384.py, 7, INFO, Writing data ((21981, 9)) to csv took 0.47231125831604004 seconds.\n",
      "2026-01-16 12:53:13,184, 807204384.py, 8, INFO, CSV file size 2.002374 MB\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "stock_prices.to_csv(stock_csv, index = False)\n",
    "end = time.time()\n",
    "\n",
    "_logs.info(f'Writing data ({stock_prices.shape}) to csv took {end - start} seconds.')\n",
    "_logs.info(f'CSV file size { os.path.getsize(stock_csv)*1e-6 } MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to Parquet\n",
    "\n",
    "### Notes on Dask \n",
    "\n",
    "We could use Pandas to save the data directly into Parquet files. However, we will use a different approach by applying the [Dask framework](https://www.dask.org/). Dask provides functionality for working with datasets that do not fit in memory and parallelization to speed up computation. A few notes on Dask and Pandas:\n",
    "\n",
    "- Pandas, Parquet, and Arrow:\n",
    "\n",
    "    + We can work with large datasets and Parquet files in Pandas, but we will generally be limited by the amount of data that can fit in our computer's memory.\n",
    "    + Pandas can write Parquet files using a PyArrow backend. In fact, recent versions of Pandas support PyArrow data types, and future versions will require a PyArrow backend. \n",
    "    + The PyArrow library is an interface between Python and the Apache Arrow project. In particular, the [Parquet data format](https://parquet.apache.org/) and [Arrow](https://arrow.apache.org/docs/python/parquet.html) are Apache projects.\n",
    "\n",
    "- Dask \n",
    "\n",
    "    + Dask is much more than an interface to Arrow: Dask provides parallel and distributed computing on Pandas-like dataframes. \n",
    "    + Dask is also relatively easy to use as it mimics Pandas' API.\n",
    "    + Dask allows us to work with larger datasets than Pandas. In a sense, it is an intermediate step between Pandas and big-data frameworks like Spark (or Databricks).\n",
    "    + If you are familiar with Pandas, a good introduction is [10 Minutes to Dask](https://docs.dask.org/en/stable/10-minutes-to-dask.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "parquet_dir = os.path.join(temp, \"parquet\")\n",
    "shutil.rmtree(parquet_dir, ignore_errors=True)\n",
    "os.makedirs(parquet_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 12:57:52,271, 529525882.py, 8, INFO, Writing dd ((21981, 9)) to parquet took 0.15759611129760742 seconds.\n",
      "2026-01-16 12:57:52,276, 529525882.py, 9, INFO, Parquet file size 0.63969 MB\n"
     ]
    }
   ],
   "source": [
    "px_dd = dd.from_pandas(stock_prices, npartitions = len(select_tickers))\n",
    "\n",
    "start = time.time()\n",
    "px_dd.to_parquet(parquet_dir, engine = \"pyarrow\")\n",
    "end = time.time()\n",
    "\n",
    "_logs.info(f'Writing dd ({stock_prices.shape}) to parquet took {end - start} seconds.')\n",
    "_logs.info(f'Parquet file size { get_dir_size(parquet_dir)*1e-6 } MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas, Dask, and Parquet\n",
    "\n",
    "The distinction of Pandas Dataframes, Dask Dataframes, and Parquet files is important:\n",
    "\n",
    "+ Pandas dataframes combine the functionality of [NumPy](https://numpy.org/) (efficient vector operations, especially vector algebra) with a concise data manipulation framework that allows us to create columns of different data types (NumPy only allows single-type matrices), database-like operations (such as filtering rows, subsetting columns, and joining different dataframes).\n",
    "+ Dask dataframes extend the functionality of Pandas dataframes beyond the confines of available memory and implement parallelized operations, among other benefits.\n",
    "+ Parquet files are a file format. Parquet files can be created by Pandas and Dask, but Dask offers a superior interface. Parquet files are immutable: once written, they cannot be modified.\n",
    "+ Parquet and Dask are not the same: Parquet is a file format that can be accessed by many applications and programming languages (Python, R, Power BI, etc.), while Dask is a Python package for working with large datasets using distributed computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask is Powerful, but not Infallible\n",
    "\n",
    "It is tempting to think of Dask as a super-Pandas, but each package has its advantages and disadvantages. \n",
    "\n",
    "+ Dask is not good at everything (see [Dask DataFrames Best Practices](https://docs.dask.org/en/stable/dataframe-best-practices.html)). \n",
    "+ A useful and somewhat idiosyncratic comparison of various data manipulation frameworks is shown below (from [DataFrames at Scale Comparison: TPC-H](https://docs.coiled.io/blog/tpch.html#when-to-use-duckdb-vs-polars-vs-dask-vs-spark)) \n",
    "\n",
    "Concept              | Spark | Dask | DuckDB | Polars\n",
    "---------------------|-------|------|--------|--------\n",
    "Fast Locally         | ‚ùå    | ü§î  | ‚úÖ     | ‚úÖ\n",
    "Fast on Cloud (1 TB) | ‚úÖ    | ‚úÖ  | ‚úÖ     | ‚ùå\n",
    "Fast on Cloud (10 TB)| ‚ùå    | ‚úÖ  | ‚úÖ     | ‚ùå\n",
    "Scales Out           | ‚úÖ    | ‚úÖ  | ‚ùå     | ‚ùå\n",
    "SQL                  | ‚úÖ    | ü§î  | ‚úÖ     | ü§î\n",
    "More than SQL        | ‚úÖ    | ‚úÖ  | ‚ùå     | ‚úÖ\n",
    "Sensible Defaults    | ‚ùå    | ‚úÖ  | ‚úÖ     | ‚úÖ\n",
    "\n",
    "\n",
    "### Dask Best Practices\n",
    "\n",
    "Parallelism brings extra complexity and overhead. Here are a few ideas to help you decide when to use Dask (from [Dask's Best Practices](https://docs.dask.org/en/stable/best-practices.html)):\n",
    "\n",
    "#### Small is Better\n",
    "\n",
    "+ Start small: if possible, use Pandas. Also, try to reduce your data using aggregation, then use Pandas.\n",
    "+ More generally, NumPy, Pandas, and Scikit-Learn may have faster functions for what you need. Consult the relevant documentation, experiment, and/or consult with a colleague or expert.\n",
    "\n",
    "#### Index with Care\n",
    "\n",
    "+ Use the index: it is beneficial to have a well-defined index in Dask DataFrames, as it may speed up searching (filtering) the data. A one-dimensional index is allowed.\n",
    "+ Minimize full-data shuffling as much as possible: indexing is an expensive operation. \n",
    "\n",
    "### Consider the Cost of Joins\n",
    "\n",
    "+ Consider cases such as small-to-large joins, where the small dataframe fits in memory, but the large one does not. The small dataframe can be Pandas, while the larger one is a Dask dataframe.\n",
    "+ Some joins are more expensive than others. \n",
    "\n",
    "    * Not expensive:\n",
    "\n",
    "        - Join a Dask DataFrame with a Pandas DataFrame.\n",
    "        - Join a Dask DataFrame with another Dask DataFrame of a single partition.\n",
    "        - Join Dask DataFrames along their indexes.\n",
    "\n",
    "    * Expensive:\n",
    "\n",
    "        - Join Dask DataFrames along columns that are not their index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Do We Store Prices?\n",
    "\n",
    "+ We can store our data as a single blob. \n",
    "\n",
    "  - This can be difficult to maintain, especially because parquet files are immutable.\n",
    "  - Using a single file, we would need to recreate the complete file any time that we update it.\n",
    "\n",
    "+ An alternative strategy is to organize data files by ticker and date: \n",
    "\n",
    "  - We can create one file per ticker and month (or any other suitable frequency). \n",
    "  - Under this approach, we would only need to recreate the latest month's file at any update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLean up before start\n",
    "PRICE_DATA = os.getenv(\"PRICE_DATA\")\n",
    "import shutil\n",
    "if os.path.exists(PRICE_DATA):\n",
    "    shutil.rmtree(PRICE_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'source',\n",
       "       'ticker'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_prices.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 13:12:23,146, 2113979629.py, 4, INFO, Processing ticker: TNC\n",
      "2026-01-16 13:12:23,170, 2113979629.py, 8, INFO, Processing year 1973 for ticker TNC.\n",
      "2026-01-16 13:12:23,185, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1973\n",
      "2026-01-16 13:12:23,330, 2113979629.py, 8, INFO, Processing year 1974 for ticker TNC.\n",
      "2026-01-16 13:12:23,366, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1974\n",
      "2026-01-16 13:12:23,477, 2113979629.py, 8, INFO, Processing year 1975 for ticker TNC.\n",
      "2026-01-16 13:12:23,490, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1975\n",
      "2026-01-16 13:12:23,556, 2113979629.py, 8, INFO, Processing year 1976 for ticker TNC.\n",
      "2026-01-16 13:12:23,579, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1976\n",
      "2026-01-16 13:12:23,679, 2113979629.py, 8, INFO, Processing year 1977 for ticker TNC.\n",
      "2026-01-16 13:12:23,696, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1977\n",
      "2026-01-16 13:12:23,780, 2113979629.py, 8, INFO, Processing year 1978 for ticker TNC.\n",
      "2026-01-16 13:12:23,802, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1978\n",
      "2026-01-16 13:12:23,892, 2113979629.py, 8, INFO, Processing year 1979 for ticker TNC.\n",
      "2026-01-16 13:12:23,917, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1979\n",
      "2026-01-16 13:12:23,999, 2113979629.py, 8, INFO, Processing year 1980 for ticker TNC.\n",
      "2026-01-16 13:12:24,019, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1980\n",
      "2026-01-16 13:12:24,086, 2113979629.py, 8, INFO, Processing year 1981 for ticker TNC.\n",
      "2026-01-16 13:12:24,102, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1981\n",
      "2026-01-16 13:12:24,169, 2113979629.py, 8, INFO, Processing year 1982 for ticker TNC.\n",
      "2026-01-16 13:12:24,187, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1982\n",
      "2026-01-16 13:12:24,266, 2113979629.py, 8, INFO, Processing year 1983 for ticker TNC.\n",
      "2026-01-16 13:12:24,298, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1983\n",
      "2026-01-16 13:12:24,396, 2113979629.py, 8, INFO, Processing year 1984 for ticker TNC.\n",
      "2026-01-16 13:12:24,410, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1984\n",
      "2026-01-16 13:12:24,480, 2113979629.py, 8, INFO, Processing year 1985 for ticker TNC.\n",
      "2026-01-16 13:12:24,496, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1985\n",
      "2026-01-16 13:12:24,587, 2113979629.py, 8, INFO, Processing year 1986 for ticker TNC.\n",
      "2026-01-16 13:12:24,604, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1986\n",
      "2026-01-16 13:12:24,686, 2113979629.py, 8, INFO, Processing year 1987 for ticker TNC.\n",
      "2026-01-16 13:12:24,701, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1987\n",
      "2026-01-16 13:12:24,777, 2113979629.py, 8, INFO, Processing year 1988 for ticker TNC.\n",
      "2026-01-16 13:12:24,797, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1988\n",
      "2026-01-16 13:12:24,859, 2113979629.py, 8, INFO, Processing year 1989 for ticker TNC.\n",
      "2026-01-16 13:12:24,876, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1989\n",
      "2026-01-16 13:12:24,939, 2113979629.py, 8, INFO, Processing year 1990 for ticker TNC.\n",
      "2026-01-16 13:12:24,953, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1990\n",
      "2026-01-16 13:12:25,026, 2113979629.py, 8, INFO, Processing year 1991 for ticker TNC.\n",
      "2026-01-16 13:12:25,043, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1991\n",
      "2026-01-16 13:12:25,102, 2113979629.py, 8, INFO, Processing year 1992 for ticker TNC.\n",
      "2026-01-16 13:12:25,118, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1992\n",
      "2026-01-16 13:12:25,288, 2113979629.py, 8, INFO, Processing year 1993 for ticker TNC.\n",
      "2026-01-16 13:12:25,309, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1993\n",
      "2026-01-16 13:12:25,377, 2113979629.py, 8, INFO, Processing year 1994 for ticker TNC.\n",
      "2026-01-16 13:12:25,389, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1994\n",
      "2026-01-16 13:12:25,459, 2113979629.py, 8, INFO, Processing year 1995 for ticker TNC.\n",
      "2026-01-16 13:12:25,476, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1995\n",
      "2026-01-16 13:12:25,540, 2113979629.py, 8, INFO, Processing year 1996 for ticker TNC.\n",
      "2026-01-16 13:12:25,557, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1996\n",
      "2026-01-16 13:12:25,628, 2113979629.py, 8, INFO, Processing year 1997 for ticker TNC.\n",
      "2026-01-16 13:12:25,642, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1997\n",
      "2026-01-16 13:12:25,701, 2113979629.py, 8, INFO, Processing year 1998 for ticker TNC.\n",
      "2026-01-16 13:12:25,720, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1998\n",
      "2026-01-16 13:12:25,793, 2113979629.py, 8, INFO, Processing year 1999 for ticker TNC.\n",
      "2026-01-16 13:12:25,808, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_1999\n",
      "2026-01-16 13:12:25,876, 2113979629.py, 8, INFO, Processing year 2000 for ticker TNC.\n",
      "2026-01-16 13:12:25,889, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2000\n",
      "2026-01-16 13:12:25,959, 2113979629.py, 8, INFO, Processing year 2001 for ticker TNC.\n",
      "2026-01-16 13:12:25,975, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2001\n",
      "2026-01-16 13:12:26,036, 2113979629.py, 8, INFO, Processing year 2002 for ticker TNC.\n",
      "2026-01-16 13:12:26,052, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2002\n",
      "2026-01-16 13:12:26,136, 2113979629.py, 8, INFO, Processing year 2003 for ticker TNC.\n",
      "2026-01-16 13:12:26,152, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2003\n",
      "2026-01-16 13:12:26,218, 2113979629.py, 8, INFO, Processing year 2004 for ticker TNC.\n",
      "2026-01-16 13:12:26,230, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2004\n",
      "2026-01-16 13:12:26,312, 2113979629.py, 8, INFO, Processing year 2005 for ticker TNC.\n",
      "2026-01-16 13:12:26,348, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2005\n",
      "2026-01-16 13:12:26,426, 2113979629.py, 8, INFO, Processing year 2006 for ticker TNC.\n",
      "2026-01-16 13:12:26,448, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2006\n",
      "2026-01-16 13:12:26,526, 2113979629.py, 8, INFO, Processing year 2007 for ticker TNC.\n",
      "2026-01-16 13:12:26,543, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2007\n",
      "2026-01-16 13:12:26,616, 2113979629.py, 8, INFO, Processing year 2008 for ticker TNC.\n",
      "2026-01-16 13:12:26,628, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2008\n",
      "2026-01-16 13:12:26,702, 2113979629.py, 8, INFO, Processing year 2009 for ticker TNC.\n",
      "2026-01-16 13:12:26,719, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2009\n",
      "2026-01-16 13:12:26,799, 2113979629.py, 8, INFO, Processing year 2010 for ticker TNC.\n",
      "2026-01-16 13:12:26,816, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2010\n",
      "2026-01-16 13:12:26,901, 2113979629.py, 8, INFO, Processing year 2011 for ticker TNC.\n",
      "2026-01-16 13:12:26,923, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2011\n",
      "2026-01-16 13:12:27,006, 2113979629.py, 8, INFO, Processing year 2012 for ticker TNC.\n",
      "2026-01-16 13:12:27,037, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2012\n",
      "2026-01-16 13:12:27,126, 2113979629.py, 8, INFO, Processing year 2013 for ticker TNC.\n",
      "2026-01-16 13:12:27,147, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2013\n",
      "2026-01-16 13:12:27,257, 2113979629.py, 8, INFO, Processing year 2014 for ticker TNC.\n",
      "2026-01-16 13:12:27,278, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2014\n",
      "2026-01-16 13:12:27,365, 2113979629.py, 8, INFO, Processing year 2015 for ticker TNC.\n",
      "2026-01-16 13:12:27,384, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2015\n",
      "2026-01-16 13:12:27,466, 2113979629.py, 8, INFO, Processing year 2016 for ticker TNC.\n",
      "2026-01-16 13:12:27,486, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2016\n",
      "2026-01-16 13:12:27,574, 2113979629.py, 8, INFO, Processing year 2017 for ticker TNC.\n",
      "2026-01-16 13:12:27,597, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2017\n",
      "2026-01-16 13:12:27,695, 2113979629.py, 8, INFO, Processing year 2018 for ticker TNC.\n",
      "2026-01-16 13:12:27,716, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2018\n",
      "2026-01-16 13:12:27,811, 2113979629.py, 8, INFO, Processing year 2019 for ticker TNC.\n",
      "2026-01-16 13:12:27,831, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2019\n",
      "2026-01-16 13:12:27,920, 2113979629.py, 8, INFO, Processing year 2020 for ticker TNC.\n",
      "2026-01-16 13:12:27,950, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/TNC\\TNC_2020\n",
      "2026-01-16 13:12:28,053, 2113979629.py, 4, INFO, Processing ticker: CBB\n",
      "2026-01-16 13:12:28,068, 2113979629.py, 8, INFO, Processing year 1980 for ticker CBB.\n",
      "2026-01-16 13:12:28,096, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1980\n",
      "2026-01-16 13:12:28,184, 2113979629.py, 8, INFO, Processing year 1981 for ticker CBB.\n",
      "2026-01-16 13:12:28,202, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1981\n",
      "2026-01-16 13:12:28,271, 2113979629.py, 8, INFO, Processing year 1982 for ticker CBB.\n",
      "2026-01-16 13:12:28,287, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1982\n",
      "2026-01-16 13:12:28,364, 2113979629.py, 8, INFO, Processing year 1983 for ticker CBB.\n",
      "2026-01-16 13:12:28,382, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1983\n",
      "2026-01-16 13:12:28,450, 2113979629.py, 8, INFO, Processing year 1984 for ticker CBB.\n",
      "2026-01-16 13:12:28,466, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1984\n",
      "2026-01-16 13:12:28,536, 2113979629.py, 8, INFO, Processing year 1985 for ticker CBB.\n",
      "2026-01-16 13:12:28,552, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1985\n",
      "2026-01-16 13:12:28,630, 2113979629.py, 8, INFO, Processing year 1986 for ticker CBB.\n",
      "2026-01-16 13:12:28,648, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1986\n",
      "2026-01-16 13:12:28,716, 2113979629.py, 8, INFO, Processing year 1987 for ticker CBB.\n",
      "2026-01-16 13:12:28,733, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1987\n",
      "2026-01-16 13:12:28,798, 2113979629.py, 8, INFO, Processing year 1988 for ticker CBB.\n",
      "2026-01-16 13:12:28,821, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1988\n",
      "2026-01-16 13:12:28,912, 2113979629.py, 8, INFO, Processing year 1989 for ticker CBB.\n",
      "2026-01-16 13:12:28,931, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1989\n",
      "2026-01-16 13:12:28,998, 2113979629.py, 8, INFO, Processing year 1990 for ticker CBB.\n",
      "2026-01-16 13:12:29,018, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1990\n",
      "2026-01-16 13:12:29,086, 2113979629.py, 8, INFO, Processing year 1991 for ticker CBB.\n",
      "2026-01-16 13:12:29,104, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1991\n",
      "2026-01-16 13:12:29,178, 2113979629.py, 8, INFO, Processing year 1992 for ticker CBB.\n",
      "2026-01-16 13:12:29,196, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1992\n",
      "2026-01-16 13:12:29,266, 2113979629.py, 8, INFO, Processing year 1993 for ticker CBB.\n",
      "2026-01-16 13:12:29,284, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1993\n",
      "2026-01-16 13:12:29,358, 2113979629.py, 8, INFO, Processing year 1994 for ticker CBB.\n",
      "2026-01-16 13:12:29,376, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1994\n",
      "2026-01-16 13:12:29,449, 2113979629.py, 8, INFO, Processing year 1995 for ticker CBB.\n",
      "2026-01-16 13:12:29,468, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1995\n",
      "2026-01-16 13:12:29,541, 2113979629.py, 8, INFO, Processing year 1996 for ticker CBB.\n",
      "2026-01-16 13:12:29,560, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1996\n",
      "2026-01-16 13:12:29,628, 2113979629.py, 8, INFO, Processing year 1997 for ticker CBB.\n",
      "2026-01-16 13:12:29,649, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1997\n",
      "2026-01-16 13:12:29,719, 2113979629.py, 8, INFO, Processing year 1998 for ticker CBB.\n",
      "2026-01-16 13:12:29,736, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1998\n",
      "2026-01-16 13:12:29,810, 2113979629.py, 8, INFO, Processing year 1999 for ticker CBB.\n",
      "2026-01-16 13:12:29,826, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_1999\n",
      "2026-01-16 13:12:29,896, 2113979629.py, 8, INFO, Processing year 2000 for ticker CBB.\n",
      "2026-01-16 13:12:29,909, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2000\n",
      "2026-01-16 13:12:29,979, 2113979629.py, 8, INFO, Processing year 2001 for ticker CBB.\n",
      "2026-01-16 13:12:29,996, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2001\n",
      "2026-01-16 13:12:30,062, 2113979629.py, 8, INFO, Processing year 2002 for ticker CBB.\n",
      "2026-01-16 13:12:30,081, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2002\n",
      "2026-01-16 13:12:30,153, 2113979629.py, 8, INFO, Processing year 2003 for ticker CBB.\n",
      "2026-01-16 13:12:30,172, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2003\n",
      "2026-01-16 13:12:30,240, 2113979629.py, 8, INFO, Processing year 2004 for ticker CBB.\n",
      "2026-01-16 13:12:30,251, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2004\n",
      "2026-01-16 13:12:30,324, 2113979629.py, 8, INFO, Processing year 2005 for ticker CBB.\n",
      "2026-01-16 13:12:30,337, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2005\n",
      "2026-01-16 13:12:30,396, 2113979629.py, 8, INFO, Processing year 2006 for ticker CBB.\n",
      "2026-01-16 13:12:30,409, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2006\n",
      "2026-01-16 13:12:30,486, 2113979629.py, 8, INFO, Processing year 2007 for ticker CBB.\n",
      "2026-01-16 13:12:30,500, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2007\n",
      "2026-01-16 13:12:30,582, 2113979629.py, 8, INFO, Processing year 2008 for ticker CBB.\n",
      "2026-01-16 13:12:30,597, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2008\n",
      "2026-01-16 13:12:30,660, 2113979629.py, 8, INFO, Processing year 2009 for ticker CBB.\n",
      "2026-01-16 13:12:30,676, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2009\n",
      "2026-01-16 13:12:30,746, 2113979629.py, 8, INFO, Processing year 2010 for ticker CBB.\n",
      "2026-01-16 13:12:30,761, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2010\n",
      "2026-01-16 13:12:30,841, 2113979629.py, 8, INFO, Processing year 2011 for ticker CBB.\n",
      "2026-01-16 13:12:30,856, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2011\n",
      "2026-01-16 13:12:30,932, 2113979629.py, 8, INFO, Processing year 2012 for ticker CBB.\n",
      "2026-01-16 13:12:30,947, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2012\n",
      "2026-01-16 13:12:31,016, 2113979629.py, 8, INFO, Processing year 2013 for ticker CBB.\n",
      "2026-01-16 13:12:31,031, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2013\n",
      "2026-01-16 13:12:31,099, 2113979629.py, 8, INFO, Processing year 2014 for ticker CBB.\n",
      "2026-01-16 13:12:31,116, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2014\n",
      "2026-01-16 13:12:31,197, 2113979629.py, 8, INFO, Processing year 2015 for ticker CBB.\n",
      "2026-01-16 13:12:31,212, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2015\n",
      "2026-01-16 13:12:31,284, 2113979629.py, 8, INFO, Processing year 2016 for ticker CBB.\n",
      "2026-01-16 13:12:31,302, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2016\n",
      "2026-01-16 13:12:31,366, 2113979629.py, 8, INFO, Processing year 2017 for ticker CBB.\n",
      "2026-01-16 13:12:31,378, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2017\n",
      "2026-01-16 13:12:31,446, 2113979629.py, 8, INFO, Processing year 2018 for ticker CBB.\n",
      "2026-01-16 13:12:31,462, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2018\n",
      "2026-01-16 13:12:31,530, 2113979629.py, 8, INFO, Processing year 2019 for ticker CBB.\n",
      "2026-01-16 13:12:31,546, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2019\n",
      "2026-01-16 13:12:31,612, 2113979629.py, 8, INFO, Processing year 2020 for ticker CBB.\n",
      "2026-01-16 13:12:31,628, 2113979629.py, 15, INFO, Writing data to path: ../../05_src/data/prices/CBB\\CBB_2020\n"
     ]
    }
   ],
   "source": [
    "for ticker in stock_prices['ticker'].unique():\n",
    "    # Filter data for ticker\n",
    "    # Notice that these are Pandas dataframes\n",
    "    _logs.info(f'Processing ticker: {ticker}')\n",
    "    ticker_dt = stock_prices[stock_prices['ticker'] == ticker]\n",
    "    ticker_dt = ticker_dt.assign(Year = ticker_dt.Date.dt.year)\n",
    "    for yr in ticker_dt['Year'].unique():\n",
    "        _logs.info(f'Processing year {yr} for ticker {ticker}.')\n",
    "        # Filter data for year and convert to Dask dataframe\n",
    "        yr_dd = dd.from_pandas(ticker_dt[ticker_dt['Year'] == yr],2)\n",
    "        \n",
    "        # Define path and create directories if not exist\n",
    "        yr_path = os.path.join(PRICE_DATA, ticker, f\"{ticker}_{yr}\")\n",
    "        os.makedirs(os.path.dirname(yr_path), exist_ok=True)\n",
    "        _logs.info(f'Writing data to path: {yr_path}')\n",
    "\n",
    "        # Write to Parquet\n",
    "        yr_dd.to_parquet(yr_path, engine = \"pyarrow\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we want to store data this way?\n",
    "\n",
    "+ Data files are easier to maintain. We do not update old data, only recent data or the most recent \"delta\".\n",
    "+ Parquet files, as long as they maintain a consistent schema, can all be read jointly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Transform, and Save\n",
    "\n",
    "In this section, we will load the Parquet files we generated, transform the data, and save the resulting dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "+ Parquet files can be read individually or as a collection.\n",
    "+ `dd.read_parquet()` can take a list (collection) of files as input.\n",
    "+ Use `glob` to obtain the collection of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 13:12:44,944, 174194153.py, 4, INFO, Found 178 parquet files for reading back into Dask.\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "parquet_files = glob(os.path.join(PRICE_DATA, \"**/*.parquet\"), recursive = True)\n",
    "_logs.info(f'Found {len(parquet_files)} parquet files for reading back into Dask.')\n",
    "\n",
    "dd_px = dd.read_parquet(parquet_files).set_index(\"ticker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "+ This transformation step will create a *Features* data set. In our case, features will be stock returns (we obtained prices).\n",
    "+ Dask dataframes work similarly to Pandas dataframes: in particular, we can perform groupby and apply operations.\n",
    "+ Notice the use of [an anonymous (lambda) function](https://realpython.com/python-lambda/) in the apply statement.\n",
    "\n",
    "In the code below, the following operation occurs:\n",
    "\n",
    "+ Start with a Dask dataframe, `dd_px`.\n",
    "+ Group the rows of this dataframe by the variable `ticker`, i.e., each group will contain the observations that pertain only to one ticker at a time. The `group_key` parameter controls whether an index entry is added with the value of the grouping variable (`ticker` in this case); if we made `group_keys=True`, we would have a duplicate `ticker` column.\n",
    "+ For each group defined by a `ticker`, `apply()` the following calculation:\n",
    "\n",
    "    - Sort the values by `Date` in ascending order.\n",
    "    - Assign a new variable called `Close_lag_1` by shifting the position of the closing price (`Close`) by one position. \n",
    "    - Define the schema of the resulting dataframe. If we omit this specification, we would get a warning; however, the simplicity of the calculation ensures that Dask can determine the resulting schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>source</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CBB</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>string</td>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNC</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNC</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: setindex, 2 expressions</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                         Date     Open     High      Low    Close Adj Close Volume  source   Year\n",
       "npartitions=2                                                                                    \n",
       "CBB            datetime64[ns]  float64  float64  float64  float64   float64  int64  string  int32\n",
       "TNC                       ...      ...      ...      ...      ...       ...    ...     ...    ...\n",
       "TNC                       ...      ...      ...      ...      ...       ...    ...     ...    ...\n",
       "Dask Name: setindex, 2 expressions\n",
       "Expr=SetIndex(frame=ReadParquetFSSpec(b5f4666), _other='ticker', options={})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_shift = (\n",
    "    dd_px\n",
    "        .groupby('ticker', group_keys=False)\n",
    "        .apply(\n",
    "            lambda x: x.sort_values('Date', ascending = True)\n",
    "                       .assign(Close_lag_1 = x['Close'].shift(1)), \n",
    "            meta = pd.DataFrame(data ={'Date': 'datetime64[ns]',\n",
    "                    'Open': 'f8',\n",
    "                    'High': 'f8',\n",
    "                    'Low': 'f8',\n",
    "                    'Close': 'f8',\n",
    "                    'Adj Close': 'f8',\n",
    "                    'Volume': 'i8',\n",
    "                    'source': 'object',\n",
    "                    'Year': 'int32',\n",
    "                    'Close_lag_1': 'f8'},\n",
    "                    index = pd.Index([], dtype=pd.StringDtype(), name='ticker'))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using the dataframe that we created above, we can now `assign` the `Returns` variable to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_rets = dd_shift.assign(\n",
    "    Returns = lambda x: x['Close']/x['Close_lag_1'] - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: How do we know that we are not (erroneously) combining the last price of a ticker with the first price of the next ticker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Exection\n",
    "\n",
    "What does `dd_rets` contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>source</th>\n",
       "      <th>Year</th>\n",
       "      <th>Close_lag_1</th>\n",
       "      <th>Returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=60</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALDX</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXI</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXI</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: assign, 9 expressions</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                  Date    Open    High     Low   Close Adj Close  Volume  source    Year Close_lag_1 Returns\n",
       "npartitions=60                                                                                              \n",
       "ACN             object  object  object  object  object    object  object  object  object      object  object\n",
       "ALDX               ...     ...     ...     ...     ...       ...     ...     ...     ...         ...     ...\n",
       "...                ...     ...     ...     ...     ...       ...     ...     ...     ...         ...     ...\n",
       "ZIXI               ...     ...     ...     ...     ...       ...     ...     ...     ...         ...     ...\n",
       "ZIXI               ...     ...     ...     ...     ...       ...     ...     ...     ...         ...     ...\n",
       "Dask Name: assign, 9 expressions\n",
       "Expr=Assign(frame=Assign(frame=GroupByApply(frame=SetIndex(frame=ReadParquetFSSpec(ec11af4), _other='ticker', options={}), observed=False, group_keys=False, func=<function <lambda> at 0x00000181C6023060>, args=(), kwargs={})))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_rets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Dask is a lazy execution framework: commands will not execute until the computation is required. \n",
    "+ To trigger an execution in dask use `.compute()` or execute a command that requires the actual values (for example, write to Parquet or SQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>source</th>\n",
       "      <th>Year</th>\n",
       "      <th>Close_lag_1</th>\n",
       "      <th>Returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>2001-07-19</td>\n",
       "      <td>15.10</td>\n",
       "      <td>15.29</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.17</td>\n",
       "      <td>11.404394</td>\n",
       "      <td>34994300.0</td>\n",
       "      <td>ACN.csv</td>\n",
       "      <td>2001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>2001-07-20</td>\n",
       "      <td>15.05</td>\n",
       "      <td>15.05</td>\n",
       "      <td>14.80</td>\n",
       "      <td>15.01</td>\n",
       "      <td>11.284108</td>\n",
       "      <td>9238500.0</td>\n",
       "      <td>ACN.csv</td>\n",
       "      <td>2001</td>\n",
       "      <td>15.17</td>\n",
       "      <td>-0.010547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>2001-07-23</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.01</td>\n",
       "      <td>14.55</td>\n",
       "      <td>15.00</td>\n",
       "      <td>11.276587</td>\n",
       "      <td>7501000.0</td>\n",
       "      <td>ACN.csv</td>\n",
       "      <td>2001</td>\n",
       "      <td>15.01</td>\n",
       "      <td>-0.000666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>2001-07-24</td>\n",
       "      <td>14.95</td>\n",
       "      <td>14.97</td>\n",
       "      <td>14.70</td>\n",
       "      <td>14.86</td>\n",
       "      <td>11.171341</td>\n",
       "      <td>3537300.0</td>\n",
       "      <td>ACN.csv</td>\n",
       "      <td>2001</td>\n",
       "      <td>15.00</td>\n",
       "      <td>-0.009333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>2001-07-25</td>\n",
       "      <td>14.70</td>\n",
       "      <td>14.95</td>\n",
       "      <td>14.65</td>\n",
       "      <td>14.95</td>\n",
       "      <td>11.238999</td>\n",
       "      <td>4208100.0</td>\n",
       "      <td>ACN.csv</td>\n",
       "      <td>2001</td>\n",
       "      <td>14.86</td>\n",
       "      <td>0.006057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXI</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>4.06</td>\n",
       "      <td>4.53</td>\n",
       "      <td>3.88</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.510000</td>\n",
       "      <td>1668500.0</td>\n",
       "      <td>ZIXI.csv</td>\n",
       "      <td>2020</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.116337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXI</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>4.49</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>1146800.0</td>\n",
       "      <td>ZIXI.csv</td>\n",
       "      <td>2020</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXI</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>4.83</td>\n",
       "      <td>4.87</td>\n",
       "      <td>4.44</td>\n",
       "      <td>4.64</td>\n",
       "      <td>4.640000</td>\n",
       "      <td>1212000.0</td>\n",
       "      <td>ZIXI.csv</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.205195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXI</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.69</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.31</td>\n",
       "      <td>4.310000</td>\n",
       "      <td>1057200.0</td>\n",
       "      <td>ZIXI.csv</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.143236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXI</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>4.11</td>\n",
       "      <td>4.16</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>539500.0</td>\n",
       "      <td>ZIXI.csv</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.010582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239659 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date   Open   High    Low  Close  Adj Close      Volume  \\\n",
       "ticker                                                                 \n",
       "ACN    2001-07-19  15.10  15.29  15.00  15.17  11.404394  34994300.0   \n",
       "ACN    2001-07-20  15.05  15.05  14.80  15.01  11.284108   9238500.0   \n",
       "ACN    2001-07-23  15.00  15.01  14.55  15.00  11.276587   7501000.0   \n",
       "ACN    2001-07-24  14.95  14.97  14.70  14.86  11.171341   3537300.0   \n",
       "ACN    2001-07-25  14.70  14.95  14.65  14.95  11.238999   4208100.0   \n",
       "...           ...    ...    ...    ...    ...        ...         ...   \n",
       "ZIXI   2020-03-26   4.06   4.53   3.88   4.51   4.510000   1668500.0   \n",
       "ZIXI   2020-03-27   4.49   4.71   4.10   4.60   4.600000   1146800.0   \n",
       "ZIXI   2020-03-30   4.83   4.87   4.44   4.64   4.640000   1212000.0   \n",
       "ZIXI   2020-03-31   4.60   4.69   4.10   4.31   4.310000   1057200.0   \n",
       "ZIXI   2020-04-01   4.11   4.16   3.80   3.82   3.820000    539500.0   \n",
       "\n",
       "          source  Year  Close_lag_1   Returns  \n",
       "ticker                                         \n",
       "ACN      ACN.csv  2001          NaN       NaN  \n",
       "ACN      ACN.csv  2001        15.17 -0.010547  \n",
       "ACN      ACN.csv  2001        15.01 -0.000666  \n",
       "ACN      ACN.csv  2001        15.00 -0.009333  \n",
       "ACN      ACN.csv  2001        14.86  0.006057  \n",
       "...          ...   ...          ...       ...  \n",
       "ZIXI    ZIXI.csv  2020         4.04  0.116337  \n",
       "ZIXI    ZIXI.csv  2020         4.00  0.150000  \n",
       "ZIXI    ZIXI.csv  2020         3.85  0.205195  \n",
       "ZIXI    ZIXI.csv  2020         3.77  0.143236  \n",
       "ZIXI    ZIXI.csv  2020         3.78  0.010582  \n",
       "\n",
       "[239659 rows x 11 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd_rets.compute()\n",
    "df.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>source</th>\n",
       "      <th>Year</th>\n",
       "      <th>Close_lag_1</th>\n",
       "      <th>Returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>2001-07-19</td>\n",
       "      <td>15.10</td>\n",
       "      <td>15.29</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.17</td>\n",
       "      <td>11.404394</td>\n",
       "      <td>34994300.0</td>\n",
       "      <td>ACN.csv</td>\n",
       "      <td>2001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>2001-07-20</td>\n",
       "      <td>15.05</td>\n",
       "      <td>15.05</td>\n",
       "      <td>14.80</td>\n",
       "      <td>15.01</td>\n",
       "      <td>11.284108</td>\n",
       "      <td>9238500.0</td>\n",
       "      <td>ACN.csv</td>\n",
       "      <td>2001</td>\n",
       "      <td>153.839996</td>\n",
       "      <td>-0.902431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>2001-07-23</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.01</td>\n",
       "      <td>14.55</td>\n",
       "      <td>15.00</td>\n",
       "      <td>11.276587</td>\n",
       "      <td>7501000.0</td>\n",
       "      <td>ACN.csv</td>\n",
       "      <td>2001</td>\n",
       "      <td>154.550003</td>\n",
       "      <td>-0.902944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>2001-07-24</td>\n",
       "      <td>14.95</td>\n",
       "      <td>14.97</td>\n",
       "      <td>14.70</td>\n",
       "      <td>14.86</td>\n",
       "      <td>11.171341</td>\n",
       "      <td>3537300.0</td>\n",
       "      <td>ACN.csv</td>\n",
       "      <td>2001</td>\n",
       "      <td>156.380005</td>\n",
       "      <td>-0.904975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>2001-07-25</td>\n",
       "      <td>14.70</td>\n",
       "      <td>14.95</td>\n",
       "      <td>14.65</td>\n",
       "      <td>14.95</td>\n",
       "      <td>11.238999</td>\n",
       "      <td>4208100.0</td>\n",
       "      <td>ACN.csv</td>\n",
       "      <td>2001</td>\n",
       "      <td>157.669998</td>\n",
       "      <td>-0.905182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date   Open   High    Low  Close  Adj Close      Volume   source  \\\n",
       "ticker                                                                          \n",
       "ACN    2001-07-19  15.10  15.29  15.00  15.17  11.404394  34994300.0  ACN.csv   \n",
       "ACN    2001-07-20  15.05  15.05  14.80  15.01  11.284108   9238500.0  ACN.csv   \n",
       "ACN    2001-07-23  15.00  15.01  14.55  15.00  11.276587   7501000.0  ACN.csv   \n",
       "ACN    2001-07-24  14.95  14.97  14.70  14.86  11.171341   3537300.0  ACN.csv   \n",
       "ACN    2001-07-25  14.70  14.95  14.65  14.95  11.238999   4208100.0  ACN.csv   \n",
       "\n",
       "        Year  Close_lag_1   Returns  \n",
       "ticker                               \n",
       "ACN     2001          NaN       NaN  \n",
       "ACN     2001   153.839996 -0.902431  \n",
       "ACN     2001   154.550003 -0.902944  \n",
       "ACN     2001   156.380005 -0.904975  \n",
       "ACN     2001   157.669998 -0.905182  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    "\n",
    "With our transformed data, we can now save the new feature to a Parquet file. We will need to answer the following questions depending on our context, setup, and available resources:\n",
    "\n",
    "+ Should we keep the same namespace? \n",
    "+ Should we save all columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>source</th>\n",
       "      <th>Year</th>\n",
       "      <th>Close_lag_1</th>\n",
       "      <th>Returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=60</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACN</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALDX</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXI</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIXI</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: assign, 9 expressions</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                  Date    Open    High     Low   Close Adj Close  Volume  source    Year Close_lag_1 Returns\n",
       "npartitions=60                                                                                              \n",
       "ACN             object  object  object  object  object    object  object  object  object      object  object\n",
       "ALDX               ...     ...     ...     ...     ...       ...     ...     ...     ...         ...     ...\n",
       "...                ...     ...     ...     ...     ...       ...     ...     ...     ...         ...     ...\n",
       "ZIXI               ...     ...     ...     ...     ...       ...     ...     ...     ...         ...     ...\n",
       "ZIXI               ...     ...     ...     ...     ...       ...     ...     ...     ...         ...     ...\n",
       "Dask Name: assign, 9 expressions\n",
       "Expr=Assign(frame=Assign(frame=GroupByApply(frame=SetIndex(frame=ReadParquetFSSpec(ec11af4), _other='ticker', options={}), observed=False, group_keys=False, func=<function <lambda> at 0x00000181C6023060>, args=(), kwargs={})))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLean up before save\n",
    "FEATURES_DATA = os.getenv(\"FEATURES_DATA\")\n",
    "if os.path.exists(FEATURES_DATA):\n",
    "    shutil.rmtree(FEATURES_DATA)\n",
    "dd_rets.to_parquet(FEATURES_DATA, \n",
    "                   overwrite = True, \n",
    "                   schema={\n",
    "                       'Date': 'timestamp[ns]',\n",
    "                       'Open': 'float64',\n",
    "                       'High': 'float64',\n",
    "                       'Low': 'float64',\n",
    "                       'Close': 'float64',\n",
    "                       'Adj Close': 'float64',\n",
    "                       'Volume': 'int64',\n",
    "                       'source': 'string',\n",
    "                       'Year': 'int32',\n",
    "                       'Close_lag_1': 'float64',\n",
    "                       'Returns': 'float64',\n",
    "                       'ticker': 'large_string'\n",
    "                   })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: from Jupyter to Command Line\n",
    "\n",
    "+ We have drafted our code in a Jupyter Notebook. \n",
    "+ Finalized code should be written in Python modules.\n",
    "\n",
    "## Object Oriented vs Functional Programming\n",
    "\n",
    "+ We can use classes to keep parameters and functions together.\n",
    "+ We *could* use Object Oriented Programming, but parallelization of data manipulation and modelling tasks benefit from *Functional Programming*.\n",
    "+ An Idea: \n",
    "\n",
    "    - [Data Oriented Programming](https://blog.klipse.tech/dop/2022/06/22/principles-of-dop.html).\n",
    "    - Use the class to bundle together parameters and functions.\n",
    "    - Use stateless operations and treat all data objects as immutable (we do not modify them, we overwrite them).\n",
    "    - Take advantage of [`@staticmethod`](https://realpython.com/instance-class-and-static-methods-demystified/).\n",
    "\n",
    "The code is in `./05_src/stock_prices/data_manager.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original design was:\n",
    "\n",
    "![](./images/02_target_pipeline_manager.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stock_prices.data_manager import DataManager\n",
    "dm = DataManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.process_sample_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add features to the data set and save to a *feature store*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.featurize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "production-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
