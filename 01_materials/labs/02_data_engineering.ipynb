{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are we doing?\n",
    "\n",
    "## Objectives \n",
    "\n",
    "\n",
    "* Build a data pipeline that downloads price data from the internet, stores it locally, transforms it into return data, and stores the feature set.\n",
    "    - Getting the data.\n",
    "    - Schemas and index in Dask.\n",
    "\n",
    "* Explore the parquet format.\n",
    "    - Reading and writing parquet files.\n",
    "    - Read datasets that are stored in distributed files.\n",
    "    - Discuss Dask vs. Pandas as a small example of big vs small data.\n",
    "    \n",
    "* Discuss the use of environment variables for settings.\n",
    "* Discuss how to use Jupyter notebooks and source code concurrently. \n",
    "* Logging and using a standard logger.\n",
    "\n",
    "## About the Data\n",
    "\n",
    "+ We will download the prices for a list of stocks.\n",
    "+ The source is Yahoo Finance, and the data, along with its description, is available via [Kaggle](https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset).\n",
    "\n",
    "\n",
    "## Medallion Architecture\n",
    "\n",
    "+ The architecture that we are thinking about is called Medallion by [DataBricks](https://www.databricks.com/glossary/medallion-architecture). It is an ELT type of thinking, although our data is well-structured.\n",
    "\n",
    "<div>\n",
    "<image src=\"./images/02_medallion_architecture.png\" height=300>\n",
    "</div>\n",
    "\n",
    "+ In our case, we would like to optimize the number of times that we download data from the internet. \n",
    "+ Ultimately, we will build a pipeline manager class to control the process of obtaining and transforming our data.\n",
    "\n",
    "<div>\n",
    "<image src=\"./images/02_target_pipeline_manager.png\" height=250>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "\n",
    "Download the [Stock Market Dataset from Kaggle](https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset). Note that you may be required to register for a free account. Alternatively, download the file from [this location](https://drive.google.com/drive/folders/1AA4gapDLpI194TGce1bY25sd91Km-tU3?usp=drive_link).\n",
    "\n",
    "+ Extract stock prices (not ETFs) into the directory: `./05_src/data/prices_csv/`. \n",
    "+ To be clear, your folder structure should include the path `05_src/data/prices_csv/stocks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command `%run update_path.py` runs a local script that adds the repository's `./05_src/` directory to the Notebook's kernel path. This way, we can use our modules within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run update_path.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the historical price data for stocks and ETFs, use the code below. Notice the following:\n",
    "\n",
    "+ Libraries are ordered from high-level to low-level libraries from the package manager. Local modules are imported at the end. \n",
    "+ The function `get_logger()` is called with `__name__` as recommended by [Python's documentation](https://docs.python.org/2/howto/logging.html#logging-advanced-tutorial).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from utils.logger import get_logger\n",
    "_logs = get_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The [`glob` module](https://docs.python.org/3/library/glob.html) is used for finding path names that match specified patterns using Unix shell-style rules.\n",
    "\n",
    "+ Notice that the module `glob` contains a function called `glob`; therefore, we used `from glob import glob` above.\n",
    "\n",
    "+ The path in which we are searching for our csv files is produced by joining two strings:\n",
    "\n",
    "\n",
    "\n",
    "    - The value of the environment variable 'SRC_DIR' that we obtain with `os.getenv('SRC_DIR')` (this variable points to ./05_src).\n",
    "\n",
    "    - Another string given by \"data/prices_csv/stocks/*.csv\".\n",
    "\n",
    "    - Both strings are combined into an OS-consistent path using `os.path.join(...)`.\n",
    "\n",
    "+ After we know the location of all our files, we sample a subset of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_files = glob(os.path.join(os.getenv('SRC_DIR'), \"data/prices_csv/stocks/*.csv\"))\n",
    "_logs.info(f'Found {len(stock_files)} stock price files.')\n",
    "\n",
    "random.seed(42)\n",
    "n_sample = 60\n",
    "stock_files = random.sample(stock_files, n_sample)\n",
    "_logs.info(f'Sampled {n_sample} stock price files for processing. The files are: {stock_files   }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the sampled files into dataframes and concatenate them:\n",
    "\n",
    "+ Start with an empty list.\n",
    "+ Read each file into a dataframe and [`append()` it to the list](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists). Notice that `append()` is an in-place operation (it does not return a list, it modifies the list in place).\n",
    "+ Finally, we concatenate all dataframes along the vertical axis (`axis=0`) using [`pd.concat()`](https://pandas.pydata.org/docs/user_guide/merging.html#concat). \n",
    "+ Notice that we do not concatenate each time that we load a dataframe. According to [Panda's documentation](https://pandas.pydata.org/docs/user_guide/merging.html#concat): \n",
    "\n",
    "> \"`concat()` makes a full copy of the data, and iteratively reusing `concat()` can create unnecessary copies. Collect all DataFrame or Series objects in a list before using `concat()`.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = []\n",
    "for s_file in stock_files:\n",
    "    _logs.info(f\"Reading file: {s_file}\")\n",
    "    dt = pd.read_csv(s_file).assign(\n",
    "        source = os.path.basename(s_file),\n",
    "        ticker = os.path.basename(s_file).replace('.csv', ''),\n",
    "        Date = lambda x: pd.to_datetime(x['Date'])\n",
    "    )\n",
    "    dt_list.append(dt)\n",
    "stock_prices = pd.concat(dt_list, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the structure of the `stock_prices` data using the [`info()` dataframe method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subset our ticker data set using standard indexing techniques. Good references for this type of data manipulation are:\n",
    "\n",
    "+ [Panda's Documentation](https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-and-selecting-data). \n",
    "+ [Panda's Cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html#cookbook-selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the subset data frame, select one column and convert to list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_tickers = stock_prices['ticker'].unique().tolist()\n",
    "select_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Data in CSV\n",
    "\n",
    "+ We have some data. How do we store it?\n",
    "+ We can compare two options, CSV and Parquet, by measuring their performance:\n",
    "\n",
    "    - Time to save: We will measure time by using the `time` library.\n",
    "    - Space required on drive: We will use the custom function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir_size(path='.'):\n",
    "    '''Returns the total size of files contained in path.'''\n",
    "    total = 0\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "            elif entry.is_dir():\n",
    "                total += get_dir_size(entry.path)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the specification of a temporary directory from the environment  and create a subdirectory in it called \"csv\":\n",
    "\n",
    "+ Use `os.getenv(\"TEMP_DATA\")` to obtain the desired location of the temporary folder from an environment variable.\n",
    "+ If the subdirectory exists, delete it using `shutil.rmtree()`; the flag `ignore_errors=True` helps us in case the subdirectory does not exist (for instance, in the first run).\n",
    "+ Create a directory with path given by `csv_dir` using `os.makedirs()`; the flag `exist_ok=True` indicates that if the directory already exists, then the function will do nothing.\n",
    "+ Finally, create the stock price file location, `stock_csv`, which will be used to create the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "temp = os.getenv(\"TEMP_DATA\")\n",
    "csv_dir = os.path.join(temp, \"csv\")\n",
    "\n",
    "shutil.rmtree(csv_dir, ignore_errors=True)\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "stock_csv = os.path.join(csv_dir, \"stock_px.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the concatenated dataframe to a CSV file. We measure the time elapsed by storing the start and end times, then we calculate their difference in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "stock_prices.to_csv(stock_csv, index = False)\n",
    "end = time.time()\n",
    "\n",
    "_logs.info(f'Writing data ({stock_prices.shape}) to csv took {end - start} seconds.')\n",
    "_logs.info(f'CSV file size { os.path.getsize(stock_csv)*1e-6 } MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to Parquet\n",
    "\n",
    "### Notes on Dask \n",
    "\n",
    "We could use Pandas to save the data directly into Parquet files. However, we will use a different approach by applying the [Dask framework](https://www.dask.org/). Dask provides functionality for working with datasets that do not fit in memory and parallelization to speed up computation. A few notes on Dask and Pandas:\n",
    "\n",
    "- Pandas, Parquet, and Arrow:\n",
    "\n",
    "    + We can work with large datasets and Parquet files in Pandas, but we will generally be limited by the amount of data that can fit in our computer's memory.\n",
    "    + Pandas can write Parquet files using a PyArrow backend. In fact, recent versions of Pandas support PyArrow data types, and future versions will require a PyArrow backend. \n",
    "    + The PyArrow library is an interface between Python and the Apache Arrow project. In particular, the [Parquet data format](https://parquet.apache.org/) and [Arrow](https://arrow.apache.org/docs/python/parquet.html) are Apache projects.\n",
    "\n",
    "- Dask \n",
    "\n",
    "    + Dask is much more than an interface to Arrow: Dask provides parallel and distributed computing on Pandas-like dataframes. \n",
    "    + Dask is also relatively easy to use as it mimics Pandas' API.\n",
    "    + Dask allows us to work with larger datasets than Pandas. In a sense, it is an intermediate step between Pandas and big-data frameworks like Spark (or Databricks).\n",
    "    + If you are familiar with Pandas, a good introduction is [10 Minutes to Dask](https://docs.dask.org/en/stable/10-minutes-to-dask.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "parquet_dir = os.path.join(temp, \"parquet\")\n",
    "shutil.rmtree(parquet_dir, ignore_errors=True)\n",
    "os.makedirs(parquet_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_dd = dd.from_pandas(stock_prices, npartitions = len(select_tickers))\n",
    "\n",
    "start = time.time()\n",
    "px_dd.to_parquet(parquet_dir, engine = \"pyarrow\")\n",
    "end = time.time()\n",
    "\n",
    "_logs.info(f'Writing dd ({stock_prices.shape}) to parquet took {end - start} seconds.')\n",
    "_logs.info(f'Parquet file size { get_dir_size(parquet_dir)*1e-6 } MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas, Dask, and Parquet\n",
    "\n",
    "The distinction of Pandas Dataframes, Dask Dataframes, and Parquet files is important:\n",
    "\n",
    "+ Pandas dataframes combine the functionality of [NumPy](https://numpy.org/) (efficient vector operations, especially vector algebra) with a concise data manipulation framework that allows us to create columns of different data types (NumPy only allows single-type matrices), database-like operations (such as filtering rows, subsetting columns, and joining different dataframes).\n",
    "+ Dask dataframes extend the functionality of Pandas dataframes beyond the confines of available memory and implement parallelized operations, among other benefits.\n",
    "+ Parquet files are a file format. Parquet files can be created by Pandas and Dask, but Dask offers a superior interface. Parquet files are immutable: once written, they cannot be modified.\n",
    "+ Parquet and Dask are not the same: Parquet is a file format that can be accessed by many applications and programming languages (Python, R, Power BI, etc.), while Dask is a Python package for working with large datasets using distributed computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask is Powerful, but not Infallible\n",
    "\n",
    "It is tempting to think of Dask as a super-Pandas, but each package has its advantages and disadvantages. \n",
    "\n",
    "+ Dask is not good at everything (see [Dask DataFrames Best Practices](https://docs.dask.org/en/stable/dataframe-best-practices.html)). \n",
    "+ A useful and somewhat idiosyncratic comparison of various data manipulation frameworks is shown below (from [DataFrames at Scale Comparison: TPC-H](https://docs.coiled.io/blog/tpch.html#when-to-use-duckdb-vs-polars-vs-dask-vs-spark)) \n",
    "\n",
    "Concept              | Spark | Dask | DuckDB | Polars\n",
    "---------------------|-------|------|--------|--------\n",
    "Fast Locally         | ‚ùå    | ü§î  | ‚úÖ     | ‚úÖ\n",
    "Fast on Cloud (1 TB) | ‚úÖ    | ‚úÖ  | ‚úÖ     | ‚ùå\n",
    "Fast on Cloud (10 TB)| ‚ùå    | ‚úÖ  | ‚úÖ     | ‚ùå\n",
    "Scales Out           | ‚úÖ    | ‚úÖ  | ‚ùå     | ‚ùå\n",
    "SQL                  | ‚úÖ    | ü§î  | ‚úÖ     | ü§î\n",
    "More than SQL        | ‚úÖ    | ‚úÖ  | ‚ùå     | ‚úÖ\n",
    "Sensible Defaults    | ‚ùå    | ‚úÖ  | ‚úÖ     | ‚úÖ\n",
    "\n",
    "\n",
    "### Dask Best Practices\n",
    "\n",
    "Parallelism brings extra complexity and overhead. Here are a few ideas to help you decide when to use Dask (from [Dask's Best Practices](https://docs.dask.org/en/stable/best-practices.html)):\n",
    "\n",
    "#### Small is Better\n",
    "\n",
    "+ Start small: if possible, use Pandas. Also, try to reduce your data using aggregation, then use Pandas.\n",
    "+ More generally, NumPy, Pandas, and Scikit-Learn may have faster functions for what you need. Consult the relevant documentation, experiment, and/or consult with a colleague or expert.\n",
    "\n",
    "#### Index with Care\n",
    "\n",
    "+ Use the index: it is beneficial to have a well-defined index in Dask DataFrames, as it may speed up searching (filtering) the data. A one-dimensional index is allowed.\n",
    "+ Minimize full-data shuffling as much as possible: indexing is an expensive operation. \n",
    "\n",
    "### Consider the Cost of Joins\n",
    "\n",
    "+ Consider cases such as small-to-large joins, where the small dataframe fits in memory, but the large one does not. The small dataframe can be Pandas, while the larger one is a Dask dataframe.\n",
    "+ Some joins are more expensive than others. \n",
    "\n",
    "    * Not expensive:\n",
    "\n",
    "        - Join a Dask DataFrame with a Pandas DataFrame.\n",
    "        - Join a Dask DataFrame with another Dask DataFrame of a single partition.\n",
    "        - Join Dask DataFrames along their indexes.\n",
    "\n",
    "    * Expensive:\n",
    "\n",
    "        - Join Dask DataFrames along columns that are not their index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Do We Store Prices?\n",
    "\n",
    "+ We can store our data as a single blob. \n",
    "\n",
    "  - This can be difficult to maintain, especially because parquet files are immutable.\n",
    "  - Using a single file, we would need to recreate the complete file any time that we update it.\n",
    "\n",
    "+ An alternative strategy is to organize data files by ticker and date: \n",
    "\n",
    "  - We can create one file per ticker and month (or any other suitable frequency). \n",
    "  - Under this approach, we would only need to recreate the latest month's file at any update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLean up before start\n",
    "PRICE_DATA = os.getenv(\"PRICE_DATA\")\n",
    "import shutil\n",
    "if os.path.exists(PRICE_DATA):\n",
    "    shutil.rmtree(PRICE_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in stock_prices['ticker'].unique():\n",
    "    # Filter data for ticker\n",
    "    # Notice that these are Pandas dataframes\n",
    "    _logs.info(f'Processing ticker: {ticker}')\n",
    "    ticker_dt = stock_prices[stock_prices['ticker'] == ticker]\n",
    "    ticker_dt = ticker_dt.assign(Year = ticker_dt.Date.dt.year)\n",
    "    for yr in ticker_dt['Year'].unique():\n",
    "        _logs.info(f'Processing year {yr} for ticker {ticker}.')\n",
    "        # Filter data for year and convert to Dask dataframe\n",
    "        yr_dd = dd.from_pandas(ticker_dt[ticker_dt['Year'] == yr],2)\n",
    "        \n",
    "        # Define path and create directories if not exist\n",
    "        yr_path = os.path.join(PRICE_DATA, ticker, f\"{ticker}_{yr}\")\n",
    "        os.makedirs(os.path.dirname(yr_path), exist_ok=True)\n",
    "        _logs.info(f'Writing data to path: {yr_path}')\n",
    "\n",
    "        # Write to Parquet\n",
    "        yr_dd.to_parquet(yr_path, engine = \"pyarrow\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we want to store data this way?\n",
    "\n",
    "+ Data files are easier to maintain. We do not update old data, only recent data or the most recent \"delta\".\n",
    "+ Parquet files, as long as they maintain a consistent schema, can all be read jointly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Transform, and Save\n",
    "\n",
    "In this section, we will load the Parquet files we generated, transform the data, and save the resulting dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "+ Parquet files can be read individually or as a collection.\n",
    "+ `dd.read_parquet()` can take a list (collection) of files as input.\n",
    "+ Use `glob` to obtain the collection of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "parquet_files = glob(os.path.join(PRICE_DATA, \"**/*.parquet\"), recursive = True)\n",
    "_logs.info(f'Found {len(parquet_files)} parquet files for reading back into Dask.')\n",
    "\n",
    "dd_px = dd.read_parquet(parquet_files).set_index(\"ticker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "+ This transformation step will create a *Features* data set. In our case, features will be stock returns (we obtained prices).\n",
    "+ Dask dataframes work similarly to Pandas dataframes: in particular, we can perform groupby and apply operations.\n",
    "+ Notice the use of [an anonymous (lambda) function](https://realpython.com/python-lambda/) in the apply statement.\n",
    "\n",
    "In the code below, the following operation occurs:\n",
    "\n",
    "+ Start with a Dask dataframe, `dd_px`.\n",
    "+ Group the rows of this dataframe by the variable `ticker`, i.e., each group will contain the observations that pertain only to one ticker at a time. The `group_key` parameter controls whether an index entry is added with the value of the grouping variable (`ticker` in this case); if we made `group_keys=True`, we would have a duplicate `ticker` column.\n",
    "+ For each group defined by a `ticker`, `apply()` the following calculation:\n",
    "\n",
    "    - Sort the values by `Date` in ascending order.\n",
    "    - Assign a new variable called `Close_lag_1` by shifting the position of the closing price (`Close`) by one position. \n",
    "    - Define the schema of the resulting dataframe. If we omit this specification, we would get a warning; however, the simplicity of the calculation ensures that Dask can determine the resulting schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_shift = (\n",
    "    dd_px\n",
    "        .groupby('ticker', group_keys=False)\n",
    "        .apply(\n",
    "            lambda x: x.sort_values('Date', ascending = True)\n",
    "                       .assign(Close_lag_1 = x['Close'].shift(1)), \n",
    "            meta = pd.DataFrame(data ={'Date': 'datetime64[ns]',\n",
    "                    'Open': 'f8',\n",
    "                    'High': 'f8',\n",
    "                    'Low': 'f8',\n",
    "                    'Close': 'f8',\n",
    "                    'Adj Close': 'f8',\n",
    "                    'Volume': 'i8',\n",
    "                    'source': 'object',\n",
    "                    'Year': 'int32',\n",
    "                    'Close_lag_1': 'f8'},\n",
    "                    index = pd.Index([], dtype=pd.StringDtype(), name='ticker'))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using the dataframe that we created above, we can now `assign` the `Returns` variable to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_rets = dd_shift.assign(\n",
    "    Returns = lambda x: x['Close']/x['Close_lag_1'] - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: How do we know that we are not (erroneously) combining the last price of a ticker with the first price of the next ticker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Exection\n",
    "\n",
    "What does `dd_rets` contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_rets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Dask is a lazy execution framework: commands will not execute until the computation is required. \n",
    "+ To trigger an execution in dask use `.compute()` or execute a command that requires the actual values (for example, write to Parquet or SQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_rets.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    "\n",
    "With our transformed data, we can now save the new feature to a Parquet file. We will need to answer the following questions depending on our context, setup, and available resources:\n",
    "\n",
    "+ Should we keep the same namespace? \n",
    "+ Should we save all columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLean up before save\n",
    "FEATURES_DATA = os.getenv(\"FEATURES_DATA\")\n",
    "if os.path.exists(FEATURES_DATA):\n",
    "    shutil.rmtree(FEATURES_DATA)\n",
    "dd_rets.to_parquet(FEATURES_DATA, \n",
    "                   overwrite = True, \n",
    "                   schema={\n",
    "                       'Date': 'timestamp[ns]',\n",
    "                       'Open': 'float64',\n",
    "                       'High': 'float64',\n",
    "                       'Low': 'float64',\n",
    "                       'Close': 'float64',\n",
    "                       'Adj Close': 'float64',\n",
    "                       'Volume': 'int64',\n",
    "                       'source': 'string',\n",
    "                       'Year': 'int32',\n",
    "                       'Close_lag_1': 'float64',\n",
    "                       'Returns': 'float64',\n",
    "                       'ticker': 'large_string'\n",
    "                   })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: from Jupyter to Command Line\n",
    "\n",
    "+ We have drafted our code in a Jupyter Notebook. \n",
    "+ Finalized code should be written in Python modules.\n",
    "\n",
    "## Object Oriented vs Functional Programming\n",
    "\n",
    "+ We can use classes to keep parameters and functions together.\n",
    "+ We *could* use Object Oriented Programming, but parallelization of data manipulation and modelling tasks benefit from *Functional Programming*.\n",
    "+ An Idea: \n",
    "\n",
    "    - [Data Oriented Programming](https://blog.klipse.tech/dop/2022/06/22/principles-of-dop.html).\n",
    "    - Use the class to bundle together parameters and functions.\n",
    "    - Use stateless operations and treat all data objects as immutable (we do not modify them, we overwrite them).\n",
    "    - Take advantage of [`@staticmethod`](https://realpython.com/instance-class-and-static-methods-demystified/).\n",
    "\n",
    "The code is in `./05_src/stock_prices/data_manager.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original design was:\n",
    "\n",
    "![](./images/02_target_pipeline_manager.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stock_prices.data_manager import DataManager\n",
    "dm = DataManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.process_sample_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add features to the data set and save to a *feature store*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.featurize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "production-env (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
