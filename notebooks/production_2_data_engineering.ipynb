{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../src/.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are we doing?\n",
    "\n",
    "## Objectives \n",
    "\n",
    "\n",
    "* Build a data pipeline that downloads price data from the internet, stores it locally, transforms it into return data, and stores the feature set.\n",
    "    - Getting the data.\n",
    "    - Schemas and index in dask.\n",
    "\n",
    "* Explore the parquet format.\n",
    "    - Reading and writing parquet files.\n",
    "    - Read datasets that are stored in distributed files.\n",
    "    - Discuss dask vs pandas as a small example of big vs small data.\n",
    "    \n",
    "* Discuss the use of environment variables for settings.\n",
    "* Discuss how to use Jupyter notebooks and source code concurrently. \n",
    "* Logging and using a standard logger.\n",
    "\n",
    "## About the Data\n",
    "\n",
    "+ We will download the prices for a list of stocks.\n",
    "+ The source is Yahoo Finance and we will use the API provided by the library yfinance.\n",
    "\n",
    "\n",
    "## Medallion Architecture\n",
    "\n",
    "+ The architecture that we are thinking about is called Medallion by [DataBricks](https://www.databricks.com/glossary/medallion-architecture). It is an ELT type of thinking, although our data is well-structured.\n",
    "\n",
    "![Medallion Architecture (DataBicks)](./img/medallion-architecture.png)\n",
    "\n",
    "+ In our case, we would like to optimize the number of times that we download data from the internet. \n",
    "+ Ultimately, we will build a pipeline manager class that will help us control the process of obtaining and transforming our data.\n",
    "\n",
    "![](./img/target_pipeline_manager.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data from Yahoo Finance\n",
    "\n",
    "Yahoo Finance provides information about public stocks in different markets. The library yfinance gives us access to a fair bit of the data in Yahoo Finance. \n",
    "\n",
    "These steps are based on the instructions in:\n",
    "\n",
    "+ [yfinance documentation](https://pypi.org/project/yfinance/)\n",
    "+ [Tutorial in geeksforgeeks.org](https://www.geeksforgeeks.org/get-financial-data-from-yahoo-finance-with-python/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ If required, install: `python -m pip install yfinance`.\n",
    "+ To download the price history of a stock, use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import yfinance as yf\n",
    "from logger import get_logger\n",
    "\n",
    "_logs = get_logger(__name__)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"yfinance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stock = yf.Ticker(\"AAPL\")\n",
    "px = stock.history(start = \"2013-12-01\", end = \"2024-02-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ If we had a few stocks, we could cycle through them. \n",
    "+ Store a csv file with a few stock tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ticker_file = os.getenv(\"TICKERS\")\n",
    "tickers = pd.read_csv(ticker_file).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 21:01:35,077, 1062139504.py, 4, INFO, Processing MSFT\n",
      "2024-02-11 21:01:35,189, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:35,190, 1062139504.py, 4, INFO, Processing AAPL\n",
      "2024-02-11 21:01:35,214, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:35,216, 1062139504.py, 4, INFO, Processing NVDA\n",
      "2024-02-11 21:01:35,315, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:35,315, 1062139504.py, 4, INFO, Processing AMZN\n",
      "2024-02-11 21:01:35,454, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:35,456, 1062139504.py, 4, INFO, Processing META\n",
      "2024-02-11 21:01:35,555, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:35,556, 1062139504.py, 4, INFO, Processing GOOGL\n",
      "2024-02-11 21:01:35,767, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:35,768, 1062139504.py, 4, INFO, Processing GOOG\n",
      "2024-02-11 21:01:35,884, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:35,886, 1062139504.py, 4, INFO, Processing BRK.B\n",
      "BRK.B: No timezone found, symbol may be delisted\n",
      "2024-02-11 21:01:36,767, 1062139504.py, 11, WARNING, No data for BRK.B\n",
      "2024-02-11 21:01:36,767, 1062139504.py, 4, INFO, Processing LLY\n",
      "2024-02-11 21:01:36,996, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:36,996, 1062139504.py, 4, INFO, Processing AVGO\n",
      "2024-02-11 21:01:37,139, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:37,140, 1062139504.py, 4, INFO, Processing TSLA\n",
      "2024-02-11 21:01:37,254, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:37,255, 1062139504.py, 4, INFO, Processing JPM\n",
      "2024-02-11 21:01:37,456, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:37,458, 1062139504.py, 4, INFO, Processing UNH\n",
      "2024-02-11 21:01:37,636, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:37,637, 1062139504.py, 4, INFO, Processing V\n",
      "2024-02-11 21:01:37,781, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:37,782, 1062139504.py, 4, INFO, Processing XOM\n",
      "2024-02-11 21:01:37,988, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:37,988, 1062139504.py, 4, INFO, Processing MA\n",
      "2024-02-11 21:01:38,127, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:38,129, 1062139504.py, 4, INFO, Processing JNJ\n",
      "2024-02-11 21:01:38,344, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:38,346, 1062139504.py, 4, INFO, Processing PG\n",
      "2024-02-11 21:01:38,587, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:38,588, 1062139504.py, 4, INFO, Processing HD\n",
      "2024-02-11 21:01:38,797, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:38,799, 1062139504.py, 4, INFO, Processing MRK\n",
      "2024-02-11 21:01:39,038, 1062139504.py, 13, INFO, Downloaded (2558, 9).\n",
      "2024-02-11 21:01:39,042, 1062139504.py, 16, INFO, Final shape (48602, 9).\n"
     ]
    }
   ],
   "source": [
    "px_list = list()\n",
    "for k, row in tickers.iterrows():\n",
    "    stock = yf.Ticker(row['ticker'])\n",
    "    _logs.info(f'Processing {row[\"ticker\"]}')\n",
    "    px = (stock\n",
    "          .history(start = pd.to_datetime(\"2013-12-01\"), \n",
    "                   end = pd.to_datetime(\"2024-02-01\"))\n",
    "          .reset_index()\n",
    "          .assign(ticker = row['ticker']))\n",
    "    if px.shape[0] == 0:\n",
    "        _logs.warning(f'No data for {row[\"ticker\"]}')\n",
    "        continue\n",
    "    _logs.info(f'Downloaded {px.shape}.')\n",
    "    px_list.append(px)\n",
    "px_dt = pd.concat(px_list, axis = 0)\n",
    "_logs.info(f'Final shape {px_dt.shape}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We have some data. How do we store it?\n",
    "+ We can compare two options: CSV and Parqruet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Data in CSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = os.getenv(\"TEMP_DATA\")\n",
    "os.makedirs(temp, exist_ok=True)\n",
    "stock_path = os.path.join(temp, \"stock_px.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 21:01:39,434, 3309402642.py, 4, INFO, Writing to dt ((48602, 9))csv took 0.35765790939331055 seconds.\n",
      "2024-02-11 21:01:39,434, 3309402642.py, 5, INFO, Csv file size 5.8630249999999995 MB\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "px_dt.to_csv(stock_path, index = False)\n",
    "end = time.time()\n",
    "_logs.info(f'Writing to dt ({px_dt.shape})csv took {end - start} seconds.')\n",
    "_logs.info(f'Csv file size { os.path.getsize(stock_path)*1e-6 } MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to Parquet\n",
    "\n",
    "## Dask \n",
    "\n",
    "We can work with with large data sets and parquet files. In fact, recent versions of pandas support pyarrow data types and future versions will require a pyarrow backend. The pyarrow library is an interface between Python and the Appache Arrow project. The [parquet data format](https://parquet.apache.org/) and [Arrow](https://arrow.apache.org/docs/python/parquet.html) are projects of the Apache Foundation.\n",
    "\n",
    "However, Dask is much more than arrow. It provides parallel and distributed computing on pandas-like dataframes. It is also relatively easy to use, bridging a gap between pandas and Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JesusCalderon\\AppData\\Local\\Temp\\ipykernel_22832\\3297237658.py:1: DeprecationWarning: The current Dask DataFrame implementation is deprecated. \n",
      "In a future release, Dask DataFrame will use new implementation that\n",
      "contains several improvements including a logical query planning.\n",
      "The user-facing DataFrame API will remain unchanged.\n",
      "\n",
      "The new implementation is already available and can be enabled by\n",
      "installing the dask-expr library:\n",
      "\n",
      "    $ pip install dask-expr\n",
      "\n",
      "and turning the query planning option on:\n",
      "\n",
      "    >>> import dask\n",
      "    >>> dask.config.set({'dataframe.query-planning': True})\n",
      "    >>> import dask.dataframe as dd\n",
      "\n",
      "API documentation for the new implementation is available at\n",
      "https://docs.dask.org/en/stable/dask-expr-api.html\n",
      "\n",
      "Any feedback can be reported on the Dask issue tracker\n",
      "https://github.com/dask/dask/issues \n",
      "\n",
      "  import dask.dataframe as dd\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 21:01:39,850, 399157755.py, 17, INFO, Writing to dd ((48602, 9)) parquet took 0.1556870937347412 seconds.\n",
      "2024-02-11 21:01:39,851, 399157755.py, 18, INFO, Parquet file size 2.242184 MB\n"
     ]
    }
   ],
   "source": [
    "def get_dir_size(path='.'):\n",
    "    total = 0\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "            elif entry.is_dir():\n",
    "                total += get_dir_size(entry.path)\n",
    "    return total\n",
    "\n",
    "\n",
    "px_dd = dd.from_pandas(px_dt, npartitions = len(tickers))\n",
    "parquet_path = os.path.join(temp, \"stock_px.parquet\")\n",
    "start = time.time()\n",
    "px_dd.to_parquet(parquet_path, engine = \"pyarrow\")\n",
    "end = time.time()\n",
    "_logs.info(f'Writing to dd ({px_dt.shape}) parquet took {end - start} seconds.')\n",
    "_logs.info(f'Parquet file size { get_dir_size(parquet_path)*1e-6 } MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we store prices?\n",
    "\n",
    "+ We can store a single blob of data. This can be difficult to maintain, especially as parquet files are immutable.\n",
    "+ Strategy: organize data files by ticker and date. Update only latest month.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRICE_DATA = os.getenv(\"PRICE_DATA\")\n",
    "for ticker in px_dt.ticker.unique():\n",
    "    ticker_dt = px_dt[px_dt.ticker == ticker]\n",
    "    ticker_dt = ticker_dt.assign(year = ticker_dt.Date.dt.year)\n",
    "    for yr in ticker_dt.year.unique():\n",
    "        yr_dt = ticker_dt[ticker_dt.year == yr]\n",
    "        yr_path = os.path.join(PRICE_DATA, ticker, f\"{ticker}_{yr}.parquet\")\n",
    "        os.makedirs(os.path.dirname(yr_path), exist_ok=True)\n",
    "        yr_dt.to_parquet(yr_path, engine = \"pyarrow\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we want to store data this way?\n",
    "\n",
    "+ Easier to maintain. We do not update old data, only recent data.\n",
    "+ We can also access all files as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Transform and Save \n",
    "\n",
    "+ Dask is a lazy execution framework: commands will not execute until they are required. \n",
    "+ To trigger an execution in dask use `.compute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "parquet_files = glob(PRICE_DATA+\"/**/*.parquet\")\n",
    "dd_px = dd.read_parquet(parquet_files).set_index(\"ticker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JesusCalderon\\AppData\\Local\\Temp\\ipykernel_22832\\3555091748.py:2: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  dd_rets = (dd_px.groupby('ticker', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dd_rets = (dd_px.groupby('ticker', group_keys=False).apply(\n",
    "    lambda x: x.assign(Close_lag_1 = x['Close'].shift(1))\n",
    ").assign(\n",
    "    log_returns = lambda x: np.log(x['Close']/x['Close_lag_1']), \n",
    "    returns = lambda x: x['Close']/x['Close_lag_1'] - 1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>year</th>\n",
       "      <th>Close_lag_1</th>\n",
       "      <th>log_returns</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>2013-12-02 00:00:00-05:00</td>\n",
       "      <td>17.448944</td>\n",
       "      <td>17.646886</td>\n",
       "      <td>17.224423</td>\n",
       "      <td>17.237244</td>\n",
       "      <td>472544800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>2013-12-03 00:00:00-05:00</td>\n",
       "      <td>17.458325</td>\n",
       "      <td>17.710991</td>\n",
       "      <td>17.438937</td>\n",
       "      <td>17.709114</td>\n",
       "      <td>450968000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>17.237244</td>\n",
       "      <td>0.027007</td>\n",
       "      <td>0.027375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>2013-12-04 00:00:00-05:00</td>\n",
       "      <td>17.683472</td>\n",
       "      <td>17.798861</td>\n",
       "      <td>17.537126</td>\n",
       "      <td>17.667837</td>\n",
       "      <td>377809600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>17.709114</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>-0.002331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>2013-12-05 00:00:00-05:00</td>\n",
       "      <td>17.907059</td>\n",
       "      <td>17.984923</td>\n",
       "      <td>17.711932</td>\n",
       "      <td>17.758524</td>\n",
       "      <td>447580000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>17.667837</td>\n",
       "      <td>0.005120</td>\n",
       "      <td>0.005133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>2013-12-06 00:00:00-05:00</td>\n",
       "      <td>17.692533</td>\n",
       "      <td>17.722554</td>\n",
       "      <td>17.498032</td>\n",
       "      <td>17.512102</td>\n",
       "      <td>344352400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>17.758524</td>\n",
       "      <td>-0.013973</td>\n",
       "      <td>-0.013876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>2024-01-25 00:00:00-05:00</td>\n",
       "      <td>100.309998</td>\n",
       "      <td>102.180000</td>\n",
       "      <td>99.620003</td>\n",
       "      <td>102.129997</td>\n",
       "      <td>22089500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>99.599998</td>\n",
       "      <td>0.025084</td>\n",
       "      <td>0.025402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>2024-01-26 00:00:00-05:00</td>\n",
       "      <td>101.970001</td>\n",
       "      <td>103.080002</td>\n",
       "      <td>101.190002</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>20817200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>102.129997</td>\n",
       "      <td>0.008483</td>\n",
       "      <td>0.008519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>2024-01-29 00:00:00-05:00</td>\n",
       "      <td>102.980003</td>\n",
       "      <td>103.199997</td>\n",
       "      <td>101.860001</td>\n",
       "      <td>103.129997</td>\n",
       "      <td>18317500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.001262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>2024-01-30 00:00:00-05:00</td>\n",
       "      <td>102.410004</td>\n",
       "      <td>104.879997</td>\n",
       "      <td>102.099998</td>\n",
       "      <td>104.849998</td>\n",
       "      <td>19610900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>103.129997</td>\n",
       "      <td>0.016540</td>\n",
       "      <td>0.016678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>2024-01-31 00:00:00-05:00</td>\n",
       "      <td>104.739998</td>\n",
       "      <td>104.879997</td>\n",
       "      <td>102.769997</td>\n",
       "      <td>102.809998</td>\n",
       "      <td>22415300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>104.849998</td>\n",
       "      <td>-0.019648</td>\n",
       "      <td>-0.019456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48602 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Date        Open        High         Low  \\\n",
       "ticker                                                                 \n",
       "AAPL   2013-12-02 00:00:00-05:00   17.448944   17.646886   17.224423   \n",
       "AAPL   2013-12-03 00:00:00-05:00   17.458325   17.710991   17.438937   \n",
       "AAPL   2013-12-04 00:00:00-05:00   17.683472   17.798861   17.537126   \n",
       "AAPL   2013-12-05 00:00:00-05:00   17.907059   17.984923   17.711932   \n",
       "AAPL   2013-12-06 00:00:00-05:00   17.692533   17.722554   17.498032   \n",
       "...                          ...         ...         ...         ...   \n",
       "XOM    2024-01-25 00:00:00-05:00  100.309998  102.180000   99.620003   \n",
       "XOM    2024-01-26 00:00:00-05:00  101.970001  103.080002  101.190002   \n",
       "XOM    2024-01-29 00:00:00-05:00  102.980003  103.199997  101.860001   \n",
       "XOM    2024-01-30 00:00:00-05:00  102.410004  104.879997  102.099998   \n",
       "XOM    2024-01-31 00:00:00-05:00  104.739998  104.879997  102.769997   \n",
       "\n",
       "             Close     Volume  Dividends  Stock Splits  year  Close_lag_1  \\\n",
       "ticker                                                                      \n",
       "AAPL     17.237244  472544800        0.0           0.0  2013          NaN   \n",
       "AAPL     17.709114  450968000        0.0           0.0  2013    17.237244   \n",
       "AAPL     17.667837  377809600        0.0           0.0  2013    17.709114   \n",
       "AAPL     17.758524  447580000        0.0           0.0  2013    17.667837   \n",
       "AAPL     17.512102  344352400        0.0           0.0  2013    17.758524   \n",
       "...            ...        ...        ...           ...   ...          ...   \n",
       "XOM     102.129997   22089500        0.0           0.0  2024    99.599998   \n",
       "XOM     103.000000   20817200        0.0           0.0  2024   102.129997   \n",
       "XOM     103.129997   18317500        0.0           0.0  2024   103.000000   \n",
       "XOM     104.849998   19610900        0.0           0.0  2024   103.129997   \n",
       "XOM     102.809998   22415300        0.0           0.0  2024   104.849998   \n",
       "\n",
       "        log_returns   returns  \n",
       "ticker                         \n",
       "AAPL            NaN       NaN  \n",
       "AAPL       0.027007  0.027375  \n",
       "AAPL      -0.002334 -0.002331  \n",
       "AAPL       0.005120  0.005133  \n",
       "AAPL      -0.013973 -0.013876  \n",
       "...             ...       ...  \n",
       "XOM        0.025084  0.025402  \n",
       "XOM        0.008483  0.008519  \n",
       "XOM        0.001261  0.001262  \n",
       "XOM        0.016540  0.016678  \n",
       "XOM       -0.019648 -0.019456  \n",
       "\n",
       "[48602 rows x 12 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_rets.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich data\n",
    "\n",
    "+ Apply transformations to calculate daily returns\n",
    "+ Store the enriched data, the silver dataset, in a new directory.\n",
    "+ Should we keep the same namespace? All columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JesusCalderon\\AppData\\Local\\Temp\\ipykernel_22832\\3505973321.py:2: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .shift(1)\n",
      "  After:  .shift(1, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .shift(1, meta=('x', 'f8'))            for series result\n",
      "  Close_lag_1 = px_dd.groupby(\"ticker\")[\"Close\"].shift(1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Not all divisions are known, can't align partitions. Please use `set_index` to set the index.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpx_dd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mClose_lag_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpx_dd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mticker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshift\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JesusCalderon\\projects\\dsi_production\\env\\Lib\\site-packages\\dask\\dataframe\\core.py:5667\u001b[0m, in \u001b[0;36mDataFrame.assign\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   5663\u001b[0m     \u001b[38;5;66;03m# Figure out columns of the output\u001b[39;00m\n\u001b[0;32m   5664\u001b[0m     df2 \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_meta_nonempty\u001b[38;5;241m.\u001b[39massign(\n\u001b[0;32m   5665\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_extract_meta({k: kwargs[k]}, nonempty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5666\u001b[0m     )\n\u001b[1;32m-> 5667\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43melemwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\JesusCalderon\\projects\\dsi_production\\env\\Lib\\site-packages\\dask\\dataframe\\core.py:6785\u001b[0m, in \u001b[0;36melemwise\u001b[1;34m(op, meta, out, transform_divisions, *args, **kwargs)\u001b[0m\n\u001b[0;32m   6781\u001b[0m args \u001b[38;5;241m=\u001b[39m _maybe_from_pandas(args)\n\u001b[0;32m   6783\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _maybe_align_partitions\n\u001b[1;32m-> 6785\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_align_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6786\u001b[0m dasks \u001b[38;5;241m=\u001b[39m [arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (_Frame, Scalar, Array))]\n\u001b[0;32m   6787\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [df \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m dasks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, _Frame)]\n",
      "File \u001b[1;32mc:\\Users\\JesusCalderon\\projects\\dsi_production\\env\\Lib\\site-packages\\dask\\dataframe\\multi.py:177\u001b[0m, in \u001b[0;36m_maybe_align_partitions\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    175\u001b[0m divisions \u001b[38;5;241m=\u001b[39m dfs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdivisions\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(df\u001b[38;5;241m.\u001b[39mdivisions \u001b[38;5;241m==\u001b[39m divisions \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m dfs):\n\u001b[1;32m--> 177\u001b[0m     dfs2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[43malign_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdfs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, _Frame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(dfs2) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mc:\\Users\\JesusCalderon\\projects\\dsi_production\\env\\Lib\\site-packages\\dask\\dataframe\\multi.py:131\u001b[0m, in \u001b[0;36malign_partitions\u001b[1;34m(*dfs)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfs contains no DataFrame and Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(df\u001b[38;5;241m.\u001b[39mknown_divisions \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m dfs1):\n\u001b[1;32m--> 131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot all divisions are known, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt align \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartitions. Please use `set_index` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto set the index.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    137\u001b[0m divisions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(unique(merge_sorted(\u001b[38;5;241m*\u001b[39m[df\u001b[38;5;241m.\u001b[39mdivisions \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m dfs1])))\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(divisions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# single value for index\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Not all divisions are known, can't align partitions. Please use `set_index` to set the index."
     ]
    }
   ],
   "source": [
    "px_dd.assign(\n",
    "    Close_lag_1 = px_dd.groupby(\"ticker\")[\"Close\"].shift(1)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
